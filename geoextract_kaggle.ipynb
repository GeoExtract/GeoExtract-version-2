{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299716f7",
   "metadata": {},
   "source": [
    "## 1 · Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f70750",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install -q \\\n",
    "    ultralytics>=8.3.0 \\\n",
    "    transformers>=4.45.0 \\\n",
    "    peft>=0.13.0 \\\n",
    "    bitsandbytes>=0.44.0 \\\n",
    "    accelerate>=1.0.0 \\\n",
    "    qwen-vl-utils \\\n",
    "    rasterio \\\n",
    "    geopandas \\\n",
    "    shapely \\\n",
    "    albumentations>=1.4.0 \\\n",
    "    wandb \\\n",
    "    scikit-learn \\\n",
    "    pyyaml \\\n",
    "    tqdm \\\n",
    "    matplotlib \\\n",
    "    pillow \\\n",
    "    opencv-python-headless\n",
    "\n",
    "print(\"✅ All dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85be305",
   "metadata": {},
   "source": [
    "## 2 · Configuration (Dynamic Paths, No Hardcoded Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, time, json, random, shutil, logging\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# ─── Environment Detection ─────────────────────\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/working\")\n",
    "\n",
    "def _resolve_root() -> Path:\n",
    "    if IS_KAGGLE:\n",
    "        return Path(\"/kaggle/working\")\n",
    "    return Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = _resolve_root()\n",
    "\n",
    "def _resolve_data_root() -> Path:\n",
    "    env = os.environ.get(\"SPACENET7_ROOT\")\n",
    "    if env:\n",
    "        return Path(env)\n",
    "    if IS_KAGGLE:\n",
    "        return Path(\"/kaggle/input/spacenet7\")\n",
    "    return PROJECT_ROOT / \"data\"\n",
    "\n",
    "DATA_ROOT = _resolve_data_root()\n",
    "\n",
    "# ─── Output Directories ────────────────────────\n",
    "OUTPUT_DIR     = PROJECT_ROOT / \"outputs\"\n",
    "YOLO_DATA_DIR  = OUTPUT_DIR / \"yolo_dataset\"\n",
    "VLM_DATA_DIR   = OUTPUT_DIR / \"vlm_dataset\"\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "YOLO_CKPT_DIR  = CHECKPOINT_DIR / \"yolo\"\n",
    "VLM_CKPT_DIR   = CHECKPOINT_DIR / \"vlm\"\n",
    "EVAL_DIR       = OUTPUT_DIR / \"evaluation\"\n",
    "EXPORT_DIR     = OUTPUT_DIR / \"export\"\n",
    "\n",
    "for _d in [YOLO_DATA_DIR, VLM_DATA_DIR, YOLO_CKPT_DIR, VLM_CKPT_DIR, EVAL_DIR, EXPORT_DIR]:\n",
    "    _d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ─── W&B Config ────────────────────────────────\n",
    "@dataclass\n",
    "class WandbConfig:\n",
    "    project: str = \"GeoExtract-v2\"\n",
    "    entity: Optional[str] = None\n",
    "    enabled: bool = True\n",
    "    api_key: Optional[str] = field(default=None, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.api_key = os.environ.get(\"WANDB_API_KEY\", self.api_key)\n",
    "        self.entity = os.environ.get(\"WANDB_ENTITY\", self.entity)\n",
    "        if not self.api_key:\n",
    "            print(\"[⚠ wandb] WANDB_API_KEY not set — logging disabled.\")\n",
    "            self.enabled = False\n",
    "\n",
    "# ─── Dataset Config ────────────────────────────\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    root: Path = DATA_ROOT\n",
    "    images_subdir: str = \"train\"\n",
    "    geojson_subdir: str = \"train\"\n",
    "    image_size: int = 640\n",
    "    val_split: float = 0.15\n",
    "    seed: int = 42\n",
    "    max_samples: Optional[int] = None\n",
    "    augment: bool = True\n",
    "    rotation_limit: int = 30\n",
    "    color_jitter: float = 0.3\n",
    "    flip_prob: float = 0.5\n",
    "\n",
    "# ─── YOLO Config ───────────────────────────────\n",
    "@dataclass\n",
    "class YOLOConfig:\n",
    "    model_variant: str = \"yolo11n.pt\"\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 16\n",
    "    image_size: int = 640\n",
    "    lr0: float = 1e-3\n",
    "    lrf: float = 0.01\n",
    "    patience: int = 10\n",
    "    save_period: int = 5\n",
    "    workers: int = 2\n",
    "    device: str = \"0\"\n",
    "    project: Path = YOLO_CKPT_DIR\n",
    "    name: str = \"building_detector\"\n",
    "    resume: bool = True\n",
    "    checkpoint_every_n_steps: int = 500\n",
    "\n",
    "# ─── VLM Config ─────────────────────────────────\n",
    "@dataclass\n",
    "class VLMConfig:\n",
    "    model_id: str = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    load_in_4bit: bool = True\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    bnb_4bit_compute_dtype: str = \"float16\"\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = field(\n",
    "        default_factory=lambda: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    epochs: int = 3\n",
    "    batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 8\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    max_seq_length: int = 1024\n",
    "    save_steps: int = 500\n",
    "    logging_steps: int = 50\n",
    "    eval_steps: int = 500\n",
    "    output_dir: Path = VLM_CKPT_DIR\n",
    "    resume_from_checkpoint: bool = True\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False\n",
    "    gradient_checkpointing: bool = True\n",
    "    device: str = \"cuda:0\"\n",
    "\n",
    "# ─── QA Config ──────────────────────────────────\n",
    "@dataclass\n",
    "class QAConfig:\n",
    "    sparse_max: int = 10\n",
    "    moderate_max: int = 30\n",
    "    dense_max: int = 50\n",
    "    min_turns: int = 2\n",
    "    max_turns: int = 4\n",
    "    system_prompt: str = (\n",
    "        \"You are GeoExtract, an expert urban planning AI that analyzes \"\n",
    "        \"satellite imagery. You provide detailed assessments of building \"\n",
    "        \"density, green space coverage, urban heat island risk, and \"\n",
    "        \"construction quality based on visual and spatial data.\"\n",
    "    )\n",
    "\n",
    "# ─── Inference Config ───────────────────────────\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    yolo_weights: Path = YOLO_CKPT_DIR / \"building_detector\" / \"weights\" / \"best.pt\"\n",
    "    vlm_adapter_dir: Path = VLM_CKPT_DIR\n",
    "    confidence_threshold: float = 0.25\n",
    "    iou_threshold: float = 0.45\n",
    "    max_new_tokens: int = 512\n",
    "    device: str = \"cuda:0\"\n",
    "\n",
    "# ─── Evaluation Config ──────────────────────────\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    iou_threshold: float = 0.5\n",
    "    density_classes: List[str] = field(\n",
    "        default_factory=lambda: [\"Sparse\", \"Moderate\", \"Dense\", \"Urban Core\"]\n",
    "    )\n",
    "    output_dir: Path = EVAL_DIR\n",
    "\n",
    "# ─── Master Config ──────────────────────────────\n",
    "@dataclass\n",
    "class GeoExtractConfig:\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    yolo: YOLOConfig = field(default_factory=YOLOConfig)\n",
    "    vlm: VLMConfig = field(default_factory=VLMConfig)\n",
    "    qa: QAConfig = field(default_factory=QAConfig)\n",
    "    inference: InferenceConfig = field(default_factory=InferenceConfig)\n",
    "    evaluation: EvalConfig = field(default_factory=EvalConfig)\n",
    "    wandb: WandbConfig = field(default_factory=WandbConfig)\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        lines = [\n",
    "            \"═\" * 55,\n",
    "            \"  GeoExtract v2 — Configuration Summary\",\n",
    "            \"═\" * 55,\n",
    "            f\"  Environment    : {'Kaggle' if IS_KAGGLE else 'Local'}\",\n",
    "            f\"  Project Root   : {PROJECT_ROOT}\",\n",
    "            f\"  Data Root      : {DATA_ROOT}\",\n",
    "            f\"  Output Dir     : {OUTPUT_DIR}\",\n",
    "            f\"  W&B Enabled    : {self.wandb.enabled}\",\n",
    "            \"─\" * 55,\n",
    "            f\"  YOLO model     : {self.yolo.model_variant}\",\n",
    "            f\"  YOLO epochs    : {self.yolo.epochs}\",\n",
    "            f\"  YOLO batch     : {self.yolo.batch_size}\",\n",
    "            \"─\" * 55,\n",
    "            f\"  VLM model      : {self.vlm.model_id}\",\n",
    "            f\"  VLM 4-bit      : {self.vlm.load_in_4bit}\",\n",
    "            f\"  LoRA r/alpha   : {self.vlm.lora_r}/{self.vlm.lora_alpha}\",\n",
    "            f\"  VLM epochs     : {self.vlm.epochs}\",\n",
    "            f\"  VLM eff. batch : {self.vlm.batch_size * self.vlm.gradient_accumulation_steps}\",\n",
    "            \"═\" * 55,\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "CFG = GeoExtractConfig()\n",
    "print(CFG.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af47e4f",
   "metadata": {},
   "source": [
    "## 3 · Utility Helpers (VRAM, Logging, Checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────── Logging ──────────────────────\n",
    "def setup_logger(name: str = \"geoextract\", level: int = logging.INFO) -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        fmt = logging.Formatter(\n",
    "            \"[%(asctime)s] %(levelname)s — %(name)s — %(message)s\",\n",
    "            datefmt=\"%H:%M:%S\",\n",
    "        )\n",
    "        handler.setFormatter(fmt)\n",
    "        logger.addHandler(handler)\n",
    "    logger.setLevel(level)\n",
    "    return logger\n",
    "\n",
    "log = setup_logger()\n",
    "\n",
    "# ──────────────── VRAM Monitoring ──────────────\n",
    "def get_vram_usage() -> dict:\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"allocated_gb\": 0, \"reserved_gb\": 0, \"total_gb\": 0}\n",
    "    return {\n",
    "        \"allocated_gb\": round(torch.cuda.memory_allocated() / 1e9, 2),\n",
    "        \"reserved_gb\": round(torch.cuda.memory_reserved() / 1e9, 2),\n",
    "        \"total_gb\": round(torch.cuda.get_device_properties(0).total_mem / 1e9, 2),\n",
    "    }\n",
    "\n",
    "def log_vram(tag: str = \"\") -> None:\n",
    "    v = get_vram_usage()\n",
    "    log.info(\n",
    "        f\"[VRAM {tag}] Allocated: {v['allocated_gb']} GB | \"\n",
    "        f\"Reserved: {v['reserved_gb']} GB | Total: {v['total_gb']} GB\"\n",
    "    )\n",
    "\n",
    "def free_vram() -> None:\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    log.info(\"[VRAM] Cache cleared.\")\n",
    "    log_vram(\"after cleanup\")\n",
    "\n",
    "# ──────────────── Checkpoints ──────────────────\n",
    "def find_latest_checkpoint(ckpt_dir: Path, prefix: str = \"checkpoint-\") -> Optional[Path]:\n",
    "    if not ckpt_dir.exists():\n",
    "        return None\n",
    "    ckpts = sorted(\n",
    "        [d for d in ckpt_dir.iterdir() if d.is_dir() and d.name.startswith(prefix)],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else 0,\n",
    "    )\n",
    "    if ckpts:\n",
    "        log.info(f\"[Checkpoint] Found {len(ckpts)} checkpoints. Latest: {ckpts[-1].name}\")\n",
    "        return ckpts[-1]\n",
    "    return None\n",
    "\n",
    "def count_parameters(model: torch.nn.Module) -> dict:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"trainable\": trainable,\n",
    "        \"trainable_pct\": round(100 * trainable / total, 2) if total > 0 else 0,\n",
    "    }\n",
    "\n",
    "# ──────────────── Timer ────────────────────────\n",
    "class Timer:\n",
    "    def __init__(self, label: str = \"Block\"):\n",
    "        self.label = label\n",
    "        self.start = 0.0\n",
    "        self.elapsed = 0.0\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed = time.time() - self.start\n",
    "        log.info(f\"[Timer] {self.label} took {self.elapsed:.1f}s\")\n",
    "\n",
    "# ──────────────── W&B Helpers ──────────────────\n",
    "def init_wandb(config, run_name: str, tags: Optional[list] = None):\n",
    "    if not config.wandb.enabled:\n",
    "        log.warning(\"[wandb] Disabled — skipping init.\")\n",
    "        return None\n",
    "    try:\n",
    "        import wandb\n",
    "        os.environ[\"WANDB_API_KEY\"] = config.wandb.api_key\n",
    "        run = wandb.init(\n",
    "            project=config.wandb.project,\n",
    "            entity=config.wandb.entity,\n",
    "            name=run_name,\n",
    "            tags=tags or [],\n",
    "            config={\n",
    "                \"yolo_model\": config.yolo.model_variant,\n",
    "                \"vlm_model\": config.vlm.model_id,\n",
    "                \"lora_r\": config.vlm.lora_r,\n",
    "            },\n",
    "            reinit=True,\n",
    "        )\n",
    "        log.info(f\"[wandb] Run '{run_name}' initialized.\")\n",
    "        return run\n",
    "    except Exception as e:\n",
    "        log.error(f\"[wandb] Init failed: {e}. Continuing without logging.\")\n",
    "        return None\n",
    "\n",
    "def finish_wandb():\n",
    "    try:\n",
    "        import wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "log_vram(\"initial\")\n",
    "print(\"✅ Utilities ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bfdb9",
   "metadata": {},
   "source": [
    "## 4 · Data Pipeline (SpaceNet 7 → YOLO Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b40a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from shapely.geometry import shape, box\n",
    "import geopandas as gpd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SpaceNet7Parser:\n",
    "    def __init__(self, cfg=CFG.data):\n",
    "        self.cfg = cfg\n",
    "        self.root = Path(cfg.root)\n",
    "        self.image_size = cfg.image_size\n",
    "        self._samples: List[Dict] = []\n",
    "\n",
    "    def discover(self) -> List[Dict]:\n",
    "        train_dir = self.root / self.cfg.images_subdir\n",
    "        if not train_dir.exists():\n",
    "            log.info(f\"[Data] Searching for alternative structures under {self.root} ...\")\n",
    "            train_dir = self.root\n",
    "            if not any(train_dir.iterdir()):\n",
    "                raise FileNotFoundError(f\"No data found at {self.root}\")\n",
    "\n",
    "        samples = []\n",
    "        aoi_dirs = sorted([d for d in train_dir.iterdir() if d.is_dir()])\n",
    "\n",
    "        for aoi_dir in aoi_dirs:\n",
    "            aoi_name = aoi_dir.name\n",
    "            images_dir = aoi_dir / \"images\"\n",
    "            labels_dir = aoi_dir / \"labels\"\n",
    "            if not images_dir.exists():\n",
    "                images_dir = aoi_dir / \"images_masked\"\n",
    "            if not images_dir.exists():\n",
    "                continue\n",
    "\n",
    "            tif_files = sorted(images_dir.glob(\"*.tif\"))\n",
    "            for tif_path in tif_files:\n",
    "                stem = tif_path.stem\n",
    "                label_candidates = [\n",
    "                    labels_dir / f\"{stem}.geojson\",\n",
    "                    labels_dir / f\"{stem}_Buildings.geojson\",\n",
    "                    labels_dir / f\"Buildings_{stem}.geojson\",\n",
    "                ]\n",
    "                label_path = None\n",
    "                for lc in label_candidates:\n",
    "                    if lc.exists():\n",
    "                        label_path = lc\n",
    "                        break\n",
    "                if label_path is None and labels_dir.exists():\n",
    "                    for gj in labels_dir.glob(\"*.geojson\"):\n",
    "                        if stem in gj.stem or gj.stem in stem:\n",
    "                            label_path = gj\n",
    "                            break\n",
    "\n",
    "                samples.append({\n",
    "                    \"image_path\": tif_path,\n",
    "                    \"label_path\": label_path,\n",
    "                    \"aoi\": aoi_name,\n",
    "                    \"timestamp\": stem,\n",
    "                    \"has_labels\": label_path is not None,\n",
    "                })\n",
    "\n",
    "        if self.cfg.max_samples and len(samples) > self.cfg.max_samples:\n",
    "            random.seed(self.cfg.seed)\n",
    "            samples = random.sample(samples, self.cfg.max_samples)\n",
    "\n",
    "        self._samples = samples\n",
    "        log.info(f\"[Data] Discovered {len(samples)} image-label pairs across {len(aoi_dirs)} AOIs.\")\n",
    "        labeled = sum(1 for s in samples if s[\"has_labels\"])\n",
    "        log.info(f\"[Data] {labeled}/{len(samples)} have GeoJSON labels.\")\n",
    "        return samples\n",
    "\n",
    "    def read_geotiff(self, path: Path) -> np.ndarray:\n",
    "        with rasterio.open(path) as src:\n",
    "            img = src.read()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        if img.shape[2] > 3:\n",
    "            img = img[:, :, :3]\n",
    "        if img.dtype != np.uint8:\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    def read_geojson(self, path: Path) -> List[Dict]:\n",
    "        if path is None or not path.exists():\n",
    "            return []\n",
    "        try:\n",
    "            gdf = gpd.read_file(path)\n",
    "            buildings = []\n",
    "            for _, row in gdf.iterrows():\n",
    "                geom = row.geometry\n",
    "                if geom is not None and geom.is_valid:\n",
    "                    buildings.append({\n",
    "                        \"geometry\": geom,\n",
    "                        \"bounds\": geom.bounds,\n",
    "                        \"area\": geom.area,\n",
    "                        \"properties\": {k: v for k, v in row.items() if k != \"geometry\"},\n",
    "                    })\n",
    "            return buildings\n",
    "        except Exception as e:\n",
    "            log.warning(f\"[Data] Failed to read {path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_image_metadata(self, path: Path) -> Dict:\n",
    "        with rasterio.open(path) as src:\n",
    "            return {\n",
    "                \"crs\": str(src.crs), \"transform\": src.transform,\n",
    "                \"bounds\": src.bounds, \"width\": src.width, \"height\": src.height,\n",
    "            }\n",
    "\n",
    "\n",
    "class YOLOFormatConverter:\n",
    "    CLASS_BUILDING = 0\n",
    "\n",
    "    def __init__(self, image_size: int = 640):\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def polygon_to_yolo_bbox(self, geometry, img_width, img_height, geo_transform=None):\n",
    "        minx, miny, maxx, maxy = geometry.bounds\n",
    "        if geo_transform is not None:\n",
    "            from rasterio.transform import rowcol\n",
    "            row_min, col_min = rowcol(geo_transform, minx, maxy)\n",
    "            row_max, col_max = rowcol(geo_transform, maxx, miny)\n",
    "            px_xmin = max(0, min(col_min, col_max))\n",
    "            px_ymin = max(0, min(row_min, row_max))\n",
    "            px_xmax = min(img_width, max(col_min, col_max))\n",
    "            px_ymax = min(img_height, max(row_min, row_max))\n",
    "        else:\n",
    "            px_xmin, px_ymin = max(0, minx), max(0, miny)\n",
    "            px_xmax, px_ymax = min(img_width, maxx), min(img_height, maxy)\n",
    "\n",
    "        bw, bh = px_xmax - px_xmin, px_ymax - px_ymin\n",
    "        if bw <= 2 or bh <= 2:\n",
    "            return None\n",
    "        x_center = np.clip((px_xmin + bw / 2) / img_width, 0.0, 1.0)\n",
    "        y_center = np.clip((px_ymin + bh / 2) / img_height, 0.0, 1.0)\n",
    "        w_norm   = np.clip(bw / img_width, 0.0, 1.0)\n",
    "        h_norm   = np.clip(bh / img_height, 0.0, 1.0)\n",
    "        return (self.CLASS_BUILDING, x_center, y_center, w_norm, h_norm)\n",
    "\n",
    "    def convert_sample(self, image_path, buildings, geo_transform=None):\n",
    "        with rasterio.open(image_path) as src:\n",
    "            img_w, img_h = src.width, src.height\n",
    "            if geo_transform is None:\n",
    "                geo_transform = src.transform\n",
    "        bboxes = []\n",
    "        for bld in buildings:\n",
    "            bbox = self.polygon_to_yolo_bbox(bld[\"geometry\"], img_w, img_h, geo_transform)\n",
    "            if bbox is not None:\n",
    "                bboxes.append(bbox)\n",
    "        return bboxes\n",
    "\n",
    "\n",
    "def build_augmentation_pipeline(cfg=CFG.data) -> A.Compose:\n",
    "    transforms = []\n",
    "    if cfg.augment:\n",
    "        transforms.extend([\n",
    "            A.HorizontalFlip(p=cfg.flip_prob),\n",
    "            A.VerticalFlip(p=cfg.flip_prob * 0.5),\n",
    "            A.RandomRotate90(p=0.3),\n",
    "            A.Rotate(limit=cfg.rotation_limit, p=0.4, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.ColorJitter(\n",
    "                brightness=cfg.color_jitter, contrast=cfg.color_jitter,\n",
    "                saturation=cfg.color_jitter * 0.5, hue=cfg.color_jitter * 0.2, p=0.5,\n",
    "            ),\n",
    "            A.RandomBrightnessContrast(p=0.3),\n",
    "            A.GaussNoise(p=0.1),\n",
    "        ])\n",
    "    transforms.append(A.Resize(cfg.image_size, cfg.image_size))\n",
    "    return A.Compose(\n",
    "        transforms,\n",
    "        bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"], min_visibility=0.3),\n",
    "    )\n",
    "\n",
    "\n",
    "class YOLODatasetBuilder:\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.parser = SpaceNet7Parser(cfg.data)\n",
    "        self.converter = YOLOFormatConverter(cfg.data.image_size)\n",
    "        self.augmenter = build_augmentation_pipeline(cfg.data)\n",
    "        self.output_dir = YOLO_DATA_DIR\n",
    "\n",
    "    def build(self) -> Path:\n",
    "        with Timer(\"YOLO Dataset Build\"):\n",
    "            samples = self.parser.discover()\n",
    "            labeled_samples = [s for s in samples if s[\"has_labels\"]]\n",
    "            if not labeled_samples:\n",
    "                raise ValueError(f\"No labeled samples in {self.cfg.data.root}\")\n",
    "\n",
    "            random.seed(self.cfg.data.seed)\n",
    "            random.shuffle(labeled_samples)\n",
    "            split_idx = int(len(labeled_samples) * (1 - self.cfg.data.val_split))\n",
    "            train_samples = labeled_samples[:split_idx]\n",
    "            val_samples = labeled_samples[split_idx:]\n",
    "            log.info(f\"[Data] Split: {len(train_samples)} train, {len(val_samples)} val\")\n",
    "\n",
    "            for split in [\"train\", \"val\"]:\n",
    "                (self.output_dir / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "                (self.output_dir / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            self._process_split(train_samples, \"train\", augment=True)\n",
    "            self._process_split(val_samples, \"val\", augment=False)\n",
    "            yaml_path = self._write_dataset_yaml()\n",
    "            log.info(f\"[Data] ✓ YOLO dataset ready at {self.output_dir}\")\n",
    "            return yaml_path\n",
    "\n",
    "    def _process_split(self, samples, split, augment):\n",
    "        img_dir = self.output_dir / \"images\" / split\n",
    "        lbl_dir = self.output_dir / \"labels\" / split\n",
    "        for sample in tqdm(samples, desc=f\"Processing {split}\"):\n",
    "            try:\n",
    "                img = self.parser.read_geotiff(sample[\"image_path\"])\n",
    "                buildings = self.parser.read_geojson(sample[\"label_path\"])\n",
    "                meta = self.parser.get_image_metadata(sample[\"image_path\"])\n",
    "                bboxes = self.converter.convert_sample(\n",
    "                    sample[\"image_path\"], buildings, meta.get(\"transform\")\n",
    "                )\n",
    "                if not bboxes:\n",
    "                    continue\n",
    "                yolo_bboxes = [(b[1], b[2], b[3], b[4]) for b in bboxes]\n",
    "                class_labels = [b[0] for b in bboxes]\n",
    "\n",
    "                if augment and self.cfg.data.augment:\n",
    "                    try:\n",
    "                        augmented = self.augmenter(\n",
    "                            image=img, bboxes=yolo_bboxes, class_labels=class_labels,\n",
    "                        )\n",
    "                        img = augmented[\"image\"]\n",
    "                        yolo_bboxes = augmented[\"bboxes\"]\n",
    "                        class_labels = augmented[\"class_labels\"]\n",
    "                    except Exception:\n",
    "                        img = cv2.resize(img, (self.cfg.data.image_size, self.cfg.data.image_size))\n",
    "                else:\n",
    "                    img = cv2.resize(img, (self.cfg.data.image_size, self.cfg.data.image_size))\n",
    "\n",
    "                if not yolo_bboxes:\n",
    "                    continue\n",
    "\n",
    "                stem = f\"{sample['aoi']}_{sample['timestamp']}\"\n",
    "                cv2.imwrite(str(img_dir / f\"{stem}.png\"), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "                with open(lbl_dir / f\"{stem}.txt\", \"w\") as f:\n",
    "                    for cls_id, bbox in zip(class_labels, yolo_bboxes):\n",
    "                        f.write(f\"{cls_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\\n\")\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[Data] Failed {sample['image_path'].name}: {e}\")\n",
    "\n",
    "    def _write_dataset_yaml(self) -> Path:\n",
    "        import yaml\n",
    "        yaml_content = {\n",
    "            \"path\": str(self.output_dir), \"train\": \"images/train\",\n",
    "            \"val\": \"images/val\", \"nc\": 1, \"names\": [\"building\"],\n",
    "        }\n",
    "        yaml_path = self.output_dir / \"dataset.yaml\"\n",
    "        with open(yaml_path, \"w\") as f:\n",
    "            yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "        return yaml_path\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        stats = {}\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            img_dir = self.output_dir / \"images\" / split\n",
    "            lbl_dir = self.output_dir / \"labels\" / split\n",
    "            n_images = len(list(img_dir.glob(\"*.png\")))\n",
    "            total_boxes = 0\n",
    "            for lbl_file in lbl_dir.glob(\"*.txt\"):\n",
    "                with open(lbl_file) as f:\n",
    "                    total_boxes += len(f.readlines())\n",
    "            stats[split] = {\n",
    "                \"images\": n_images, \"total_bboxes\": total_boxes,\n",
    "                \"avg_bboxes_per_image\": round(total_boxes / max(n_images, 1), 1),\n",
    "            }\n",
    "        return stats\n",
    "\n",
    "print(\"✅ Data pipeline ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba067da6",
   "metadata": {},
   "source": [
    "## 5 · Synthetic QA Generator (ChatML Conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3136cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityClassifier:\n",
    "    def __init__(self, cfg=CFG.qa):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def classify(self, building_count: int, image_area_m2: Optional[float] = None) -> Dict:\n",
    "        if building_count <= self.cfg.sparse_max:\n",
    "            density_class, density_desc = \"Sparse\", \"Low-density suburban or rural area\"\n",
    "            heat_risk, green_space = \"Low\", \"Abundant — large open and vegetated areas visible\"\n",
    "            livability = \"High — spacious residential environment\"\n",
    "            construction_intensity = \"Minimal\"\n",
    "        elif building_count <= self.cfg.moderate_max:\n",
    "            density_class, density_desc = \"Moderate\", \"Moderate suburban density with mixed land use\"\n",
    "            heat_risk, green_space = \"Moderate\", \"Moderate — some green patches between structures\"\n",
    "            livability = \"Good — balanced density with accessible open areas\"\n",
    "            construction_intensity = \"Active — ongoing development likely\"\n",
    "        elif building_count <= self.cfg.dense_max:\n",
    "            density_class, density_desc = \"Dense\", \"High-density urban area with tightly packed structures\"\n",
    "            heat_risk, green_space = \"High\", \"Limited — minimal vegetation corridors\"\n",
    "            livability = \"Moderate — constrained but functional residential zones\"\n",
    "            construction_intensity = \"High — significant built-up coverage\"\n",
    "        else:\n",
    "            density_class, density_desc = \"Urban Core\", \"Hyper-dense urban core with maximum building coverage\"\n",
    "            heat_risk, green_space = \"Very High\", \"Severely depleted — critical lack of vegetation\"\n",
    "            livability = \"Low — crowded environment with limited open space\"\n",
    "            construction_intensity = \"Maximum — near-complete land coverage\"\n",
    "\n",
    "        buildings_per_hectare = building_count / max(1, (image_area_m2 or 409600) / 10000)\n",
    "        return {\n",
    "            \"building_count\": building_count, \"density_class\": density_class,\n",
    "            \"density_description\": density_desc, \"heat_island_risk\": heat_risk,\n",
    "            \"green_space_assessment\": green_space, \"livability_rating\": livability,\n",
    "            \"construction_intensity\": construction_intensity,\n",
    "            \"buildings_per_hectare\": round(buildings_per_hectare, 1),\n",
    "        }\n",
    "\n",
    "\n",
    "# ──── QA Templates ─────────────────────────────\n",
    "def _q_density_analysis(info):\n",
    "    return (\n",
    "        \"Analyze the building density in this satellite image. What type of urban zone does this represent?\",\n",
    "        f\"This area shows a **{info['density_class']}** density pattern with approximately \"\n",
    "        f\"{info['building_count']} buildings detected. {info['density_description']}. \"\n",
    "        f\"The estimated building density is {info['buildings_per_hectare']} buildings per hectare.\"\n",
    "    )\n",
    "\n",
    "def _q_heat_island(info):\n",
    "    return (\n",
    "        \"What is the urban heat island risk for this area based on the visible building coverage?\",\n",
    "        f\"The urban heat island risk is **{info['heat_island_risk']}**. With {info['building_count']} \"\n",
    "        f\"structures detected, the built-up area significantly \"\n",
    "        f\"{'increases' if info['building_count'] > 30 else 'modestly affects'} surface temperature \"\n",
    "        f\"relative to surrounding undeveloped land. Construction intensity: {info['construction_intensity']}.\"\n",
    "    )\n",
    "\n",
    "def _q_green_space(info):\n",
    "    return (\n",
    "        \"Assess the green space availability and environmental health of this area.\",\n",
    "        f\"Green space assessment: {info['green_space_assessment']}. In this \"\n",
    "        f\"{info['density_class'].lower()}-density zone, vegetation coverage \"\n",
    "        f\"{'provides adequate cooling and biodiversity corridors' if info['building_count'] <= 20 else 'is insufficient for effective microclimate regulation'}. \"\n",
    "        f\"Recommendation: {'Maintain current balance' if info['building_count'] <= 20 else 'Prioritize urban greening initiatives and rooftop gardens'}.\"\n",
    "    )\n",
    "\n",
    "def _q_livability(info):\n",
    "    return (\n",
    "        \"Rate the residential livability of this zone. Would you recommend it for new housing development?\",\n",
    "        f\"Livability rating: {info['livability_rating']}. With a {info['density_class'].lower()} \"\n",
    "        f\"building density of {info['buildings_per_hectare']} structures per hectare, \"\n",
    "        f\"{'this area has capacity for additional development while maintaining quality of life' if info['building_count'] <= 25 else 'further development should be carefully planned to avoid overcrowding and infrastructure strain'}.\"\n",
    "    )\n",
    "\n",
    "def _q_construction_trend(info):\n",
    "    return (\n",
    "        \"What can you tell about the construction activity and urban growth pattern in this area?\",\n",
    "        f\"Construction intensity: {info['construction_intensity']}. The {info['building_count']} \"\n",
    "        f\"detected structures suggest \"\n",
    "        f\"{'an early-stage development area with significant growth potential' if info['building_count'] <= 15 else 'a mature built environment' if info['building_count'] > 40 else 'an actively developing zone in mid-growth phase'}. \"\n",
    "        f\"The spatial distribution indicates \"\n",
    "        f\"{'organic/informal growth patterns' if info['building_count'] > 45 else 'planned development with identifiable street grids'}.\"\n",
    "    )\n",
    "\n",
    "def _q_infrastructure(info):\n",
    "    return (\n",
    "        \"Based on the building density and layout, what infrastructure challenges might this area face?\",\n",
    "        f\"With {info['building_count']} buildings in this tile, key infrastructure considerations include: \"\n",
    "        f\"{'Water and sewage — adequate capacity likely available' if info['building_count'] <= 20 else 'Water and sewage — systems may be at or near capacity'}. \"\n",
    "        f\"{'Road network — sufficient for current density' if info['building_count'] <= 30 else 'Road network — congestion risk is elevated'}. \"\n",
    "        f\"{'Power grid — standard residential load' if info['building_count'] <= 25 else 'Power grid — peak demand management needed'}. \"\n",
    "        f\"Overall infrastructure stress: {'Low' if info['building_count'] <= 15 else 'Moderate' if info['building_count'] <= 35 else 'High' if info['building_count'] <= 50 else 'Critical'}.\"\n",
    "    )\n",
    "\n",
    "def _q_planning_recommendation(info):\n",
    "    return (\n",
    "        \"If you were an urban planner, what would you recommend for this area's future development?\",\n",
    "        f\"For this {info['density_class'].lower()}-density area ({info['building_count']} structures), \"\n",
    "        f\"I recommend: \"\n",
    "        f\"{'1) Controlled expansion with green buffer zones, 2) Mixed-use zoning, 3) Investment in public transit corridors' if info['building_count'] <= 25 else '1) Densification limits, 2) Mandatory green space ratios, 3) Stormwater management infrastructure upgrades' if info['building_count'] <= 45 else '1) Construction moratorium until infrastructure catches up, 2) Energy efficiency retrofitting, 3) Creating pocket parks to combat heat island effects'}.\"\n",
    "    )\n",
    "\n",
    "def _q_environmental_impact(info):\n",
    "    return (\n",
    "        \"What is the environmental footprint of this built-up area? Discuss carbon implications and ecological connectivity.\",\n",
    "        f\"Environmental analysis for {info['density_class']} zone ({info['building_count']} structures): \"\n",
    "        f\"Carbon footprint: {'Low — minimal impervious surface' if info['building_count'] <= 10 else 'Moderate — significant impervious surfaces' if info['building_count'] <= 30 else 'High — extensive land sealing'}. \"\n",
    "        f\"Ecological connectivity: {'Intact — wildlife corridors preserved' if info['building_count'] <= 15 else 'Fragmented — habitat patches isolated' if info['building_count'] <= 40 else 'Severely disrupted — near-complete habitat loss'}. \"\n",
    "        f\"Stormwater: {'Natural infiltration adequate' if info['building_count'] <= 20 else 'Engineered drainage required to prevent flooding'}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "QA_TEMPLATES = [\n",
    "    _q_density_analysis, _q_heat_island, _q_green_space, _q_livability,\n",
    "    _q_construction_trend, _q_infrastructure, _q_planning_recommendation,\n",
    "    _q_environmental_impact,\n",
    "]\n",
    "\n",
    "\n",
    "class SyntheticQAGenerator:\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.qa_cfg = cfg.qa\n",
    "        self.parser = SpaceNet7Parser(cfg.data)\n",
    "        self.classifier = DensityClassifier(cfg.qa)\n",
    "        self.output_dir = VLM_DATA_DIR\n",
    "\n",
    "    def generate(self) -> Path:\n",
    "        with Timer(\"VLM QA Generation\"):\n",
    "            samples = self.parser.discover()\n",
    "            labeled = [s for s in samples if s[\"has_labels\"]]\n",
    "            conversations = []\n",
    "            for sample in tqdm(labeled, desc=\"Generating QA pairs\"):\n",
    "                try:\n",
    "                    conv = self._generate_conversation(sample)\n",
    "                    if conv:\n",
    "                        conversations.append(conv)\n",
    "                except Exception as e:\n",
    "                    log.warning(f\"[QA] Failed for {sample['image_path'].name}: {e}\")\n",
    "\n",
    "            random.seed(self.cfg.data.seed)\n",
    "            random.shuffle(conversations)\n",
    "            split_idx = int(len(conversations) * (1 - self.cfg.data.val_split))\n",
    "            train_path = self._save_jsonl(conversations[:split_idx], \"train.jsonl\")\n",
    "            self._save_jsonl(conversations[split_idx:], \"val.jsonl\")\n",
    "            log.info(f\"[QA] Generated {split_idx} train, {len(conversations) - split_idx} val conversations.\")\n",
    "            return train_path\n",
    "\n",
    "    def _generate_conversation(self, sample):\n",
    "        buildings = self.parser.read_geojson(sample[\"label_path\"])\n",
    "        building_count = len(buildings)\n",
    "        total_area = sum(b.get(\"area\", 0) for b in buildings) if buildings else None\n",
    "        density_info = self.classifier.classify(building_count, total_area)\n",
    "\n",
    "        n_turns = random.randint(self.qa_cfg.min_turns, self.qa_cfg.max_turns)\n",
    "        selected_templates = random.sample(QA_TEMPLATES, min(n_turns, len(QA_TEMPLATES)))\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": self.qa_cfg.system_prompt}]\n",
    "        for template_fn in selected_templates:\n",
    "            question, answer = template_fn(density_info)\n",
    "            messages.append({\"role\": \"user\", \"content\": question})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        return {\n",
    "            \"id\": f\"{sample['aoi']}_{sample['timestamp']}\",\n",
    "            \"image\": str(sample[\"image_path\"]),\n",
    "            \"building_count\": building_count,\n",
    "            \"density_class\": density_info[\"density_class\"],\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "\n",
    "    def _save_jsonl(self, conversations, filename):\n",
    "        path = self.output_dir / filename\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            for conv in conversations:\n",
    "                f.write(json.dumps(conv, default=str) + \"\\n\")\n",
    "        log.info(f\"[QA] Saved {len(conversations)} conversations to {path}\")\n",
    "        return path\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        stats = {}\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            path = self.output_dir / f\"{split}.jsonl\"\n",
    "            if path.exists():\n",
    "                with open(path) as f:\n",
    "                    convs = [json.loads(l) for l in f.readlines()]\n",
    "                density_dist = {}\n",
    "                total_turns = 0\n",
    "                for c in convs:\n",
    "                    dc = c.get(\"density_class\", \"Unknown\")\n",
    "                    density_dist[dc] = density_dist.get(dc, 0) + 1\n",
    "                    total_turns += len([m for m in c[\"messages\"] if m[\"role\"] == \"user\"])\n",
    "                stats[split] = {\n",
    "                    \"conversations\": len(convs),\n",
    "                    \"total_qa_turns\": total_turns,\n",
    "                    \"density_distribution\": density_dist,\n",
    "                }\n",
    "        return stats\n",
    "\n",
    "print(\"✅ QA generator ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a61aa",
   "metadata": {},
   "source": [
    "## 6 · YOLO Building Detector Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOTrainer:\n",
    "    def __init__(self, dataset_yaml: Path, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.yolo_cfg = cfg.yolo\n",
    "        self.dataset_yaml = dataset_yaml\n",
    "        self.model = None\n",
    "        self._wandb_run = None\n",
    "\n",
    "    def train(self) -> Path:\n",
    "        from ultralytics import YOLO\n",
    "        with Timer(\"YOLO Training\"):\n",
    "            log_vram(\"before YOLO load\")\n",
    "            resume_weights = self._find_resume_weights()\n",
    "            if resume_weights and self.yolo_cfg.resume:\n",
    "                log.info(f\"[YOLO] Resuming from: {resume_weights}\")\n",
    "                self.model = YOLO(str(resume_weights))\n",
    "            else:\n",
    "                log.info(f\"[YOLO] Starting fresh with {self.yolo_cfg.model_variant}\")\n",
    "                self.model = YOLO(self.yolo_cfg.model_variant)\n",
    "            log_vram(\"after YOLO load\")\n",
    "\n",
    "            self._wandb_run = init_wandb(\n",
    "                self.cfg, run_name=\"yolo-building-detector\", tags=[\"yolo\", \"training\"]\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                results = self.model.train(\n",
    "                    data=str(self.dataset_yaml),\n",
    "                    epochs=self.yolo_cfg.epochs,\n",
    "                    batch=self.yolo_cfg.batch_size,\n",
    "                    imgsz=self.yolo_cfg.image_size,\n",
    "                    lr0=self.yolo_cfg.lr0, lrf=self.yolo_cfg.lrf,\n",
    "                    patience=self.yolo_cfg.patience,\n",
    "                    save_period=self.yolo_cfg.save_period,\n",
    "                    workers=self.yolo_cfg.workers,\n",
    "                    device=self.yolo_cfg.device,\n",
    "                    project=str(self.yolo_cfg.project),\n",
    "                    name=self.yolo_cfg.name, exist_ok=True,\n",
    "                    pretrained=True, verbose=True,\n",
    "                    hsv_h=0.015, hsv_s=0.4, hsv_v=0.3,\n",
    "                    flipud=0.3, fliplr=0.5, mosaic=0.8, mixup=0.1,\n",
    "                    plots=True, val=True,\n",
    "                )\n",
    "            except KeyboardInterrupt:\n",
    "                log.warning(\"[YOLO] Training interrupted. Weights are saved.\")\n",
    "            finally:\n",
    "                finish_wandb()\n",
    "\n",
    "            best_weights = self._get_best_weights()\n",
    "            log.info(f\"[YOLO] ✓ Best weights: {best_weights}\")\n",
    "            log_vram(\"after YOLO training\")\n",
    "            return best_weights\n",
    "\n",
    "    def validate(self) -> Dict:\n",
    "        from ultralytics import YOLO\n",
    "        best = self._get_best_weights()\n",
    "        if not best.exists():\n",
    "            log.error(\"[YOLO] No trained weights found.\")\n",
    "            return {}\n",
    "        model = YOLO(str(best))\n",
    "        results = model.val(\n",
    "            data=str(self.dataset_yaml), batch=self.yolo_cfg.batch_size,\n",
    "            imgsz=self.yolo_cfg.image_size, device=self.yolo_cfg.device,\n",
    "        )\n",
    "        metrics = {\n",
    "            \"mAP50\": results.box.map50 if hasattr(results.box, 'map50') else 0.0,\n",
    "            \"mAP50-95\": results.box.map if hasattr(results.box, 'map') else 0.0,\n",
    "            \"precision\": results.box.mp if hasattr(results.box, 'mp') else 0.0,\n",
    "            \"recall\": results.box.mr if hasattr(results.box, 'mr') else 0.0,\n",
    "        }\n",
    "        metrics[\"f1\"] = 2 * metrics[\"precision\"] * metrics[\"recall\"] / max(metrics[\"precision\"] + metrics[\"recall\"], 1e-6)\n",
    "        log.info(f\"[YOLO] Validation metrics: {metrics}\")\n",
    "        return metrics\n",
    "\n",
    "    def export_for_deployment(self, format=\"onnx\") -> Path:\n",
    "        from ultralytics import YOLO\n",
    "        best = self._get_best_weights()\n",
    "        model = YOLO(str(best))\n",
    "        return Path(model.export(format=format, imgsz=self.yolo_cfg.image_size))\n",
    "\n",
    "    def _find_resume_weights(self):\n",
    "        run_dir = self.yolo_cfg.project / self.yolo_cfg.name\n",
    "        last_weights = run_dir / \"weights\" / \"last.pt\"\n",
    "        if last_weights.exists():\n",
    "            return last_weights\n",
    "        weights_dir = run_dir / \"weights\"\n",
    "        if weights_dir.exists():\n",
    "            pts = sorted(weights_dir.glob(\"epoch*.pt\"))\n",
    "            if pts:\n",
    "                return pts[-1]\n",
    "        return None\n",
    "\n",
    "    def _get_best_weights(self) -> Path:\n",
    "        return self.yolo_cfg.project / self.yolo_cfg.name / \"weights\" / \"best.pt\"\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "        free_vram()\n",
    "        log.info(\"[YOLO] Model unloaded, VRAM freed.\")\n",
    "\n",
    "print(\"✅ YOLO trainer ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf72c96",
   "metadata": {},
   "source": [
    "## 7 · VLM Trainer (Qwen2-VL + LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "\n",
    "class GeoExtractVLMDataset(TorchDataset):\n",
    "    def __init__(self, jsonl_path: Path, processor, max_length: int = 1024, include_images: bool = True):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.include_images = include_images\n",
    "        self.conversations = []\n",
    "        with open(jsonl_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    self.conversations.append(json.loads(line))\n",
    "        log.info(f\"[VLM Dataset] Loaded {len(self.conversations)} conversations from {jsonl_path.name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conv = self.conversations[idx]\n",
    "        messages = conv[\"messages\"]\n",
    "        image_path = conv.get(\"image\")\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            if role == \"user\" and self.include_images and image_path:\n",
    "                formatted_messages.append({\n",
    "                    \"role\": role,\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image_path},\n",
    "                        {\"type\": \"text\", \"text\": content},\n",
    "                    ],\n",
    "                })\n",
    "                image_path = None\n",
    "            else:\n",
    "                formatted_messages.append({\"role\": role, \"content\": [{\"type\": \"text\", \"text\": content}]})\n",
    "        return {\"messages\": formatted_messages, \"id\": conv.get(\"id\", str(idx))}\n",
    "\n",
    "\n",
    "class ChatMLCollator:\n",
    "    def __init__(self, processor, max_length: int = 1024):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = processor.tokenizer if hasattr(processor, 'tokenizer') else processor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        texts, images = [], []\n",
    "        for sample in batch:\n",
    "            messages = sample[\"messages\"]\n",
    "            try:\n",
    "                text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            except Exception:\n",
    "                text = self._manual_chatml(messages)\n",
    "            texts.append(text)\n",
    "            for msg in messages:\n",
    "                if isinstance(msg.get(\"content\"), list):\n",
    "                    for part in msg[\"content\"]:\n",
    "                        if isinstance(part, dict) and part.get(\"type\") == \"image\":\n",
    "                            img_path = part.get(\"image\", \"\")\n",
    "                            if img_path and Path(img_path).exists():\n",
    "                                from PIL import Image\n",
    "                                try:\n",
    "                                    images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "                                except Exception:\n",
    "                                    pass\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            texts, padding=True, truncation=True,\n",
    "            max_length=self.max_length, return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = encoding[\"input_ids\"].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        labels = self._mask_non_assistant_tokens(texts, labels)\n",
    "        encoding[\"labels\"] = labels\n",
    "        return encoding\n",
    "\n",
    "    def _mask_non_assistant_tokens(self, texts, labels):\n",
    "        for i, text in enumerate(texts):\n",
    "            assistant_start_token = \"<|im_start|>assistant\"\n",
    "            assistant_end_token = \"<|im_end|>\"\n",
    "            char_pos = 0\n",
    "            assistant_ranges = []\n",
    "            while True:\n",
    "                start_idx = text.find(assistant_start_token, char_pos)\n",
    "                if start_idx == -1:\n",
    "                    break\n",
    "                content_start = text.find(\"\\n\", start_idx)\n",
    "                if content_start == -1:\n",
    "                    break\n",
    "                content_start += 1\n",
    "                end_idx = text.find(assistant_end_token, content_start)\n",
    "                if end_idx == -1:\n",
    "                    end_idx = len(text)\n",
    "                assistant_ranges.append((content_start, end_idx))\n",
    "                char_pos = end_idx + len(assistant_end_token)\n",
    "\n",
    "            if assistant_ranges:\n",
    "                mask = torch.ones_like(labels[i], dtype=torch.bool)\n",
    "                for start, end in assistant_ranges:\n",
    "                    prefix_tokens = self.tokenizer.encode(text[:start], add_special_tokens=False)\n",
    "                    content_tokens = self.tokenizer.encode(text[start:end], add_special_tokens=False)\n",
    "                    tok_start = min(len(prefix_tokens), labels.shape[1] - 1)\n",
    "                    tok_end = min(len(prefix_tokens) + len(content_tokens), labels.shape[1])\n",
    "                    mask[tok_start:tok_end] = False\n",
    "                labels[i][mask] = -100\n",
    "        return labels\n",
    "\n",
    "    def _manual_chatml(self, messages):\n",
    "        parts = []\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            if isinstance(msg[\"content\"], list):\n",
    "                content = \" \".join(\n",
    "                    p[\"text\"] for p in msg[\"content\"]\n",
    "                    if isinstance(p, dict) and p.get(\"type\") == \"text\"\n",
    "                )\n",
    "            else:\n",
    "                content = msg[\"content\"]\n",
    "            parts.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\")\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "class VLMTrainer:\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.vlm_cfg = cfg.vlm\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def train(self) -> Path:\n",
    "        with Timer(\"VLM Training\"):\n",
    "            log_vram(\"before VLM load\")\n",
    "            self._load_model()\n",
    "            log_vram(\"after VLM load\")\n",
    "            self._apply_lora()\n",
    "            train_dataset, val_dataset = self._load_datasets()\n",
    "            self._setup_trainer(train_dataset, val_dataset)\n",
    "            self._run_training()\n",
    "            final_path = self._save_final()\n",
    "            log.info(f\"[VLM] ✓ Training complete. Adapter saved to {final_path}\")\n",
    "            return final_path\n",
    "\n",
    "    def _load_model(self):\n",
    "        from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=self.vlm_cfg.load_in_4bit,\n",
    "            bnb_4bit_quant_type=self.vlm_cfg.bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=getattr(torch, self.vlm_cfg.bnb_4bit_compute_dtype),\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        log.info(f\"[VLM] Loading {self.vlm_cfg.model_id} in 4-bit NF4...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.vlm_cfg.model_id, quantization_config=bnb_config,\n",
    "            device_map=\"auto\", trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            attn_implementation=\"flash_attention_2\"\n",
    "            if torch.cuda.get_device_capability()[0] >= 8 else \"eager\",\n",
    "        )\n",
    "        self.processor = AutoProcessor.from_pretrained(self.vlm_cfg.model_id, trust_remote_code=True)\n",
    "        if self.processor.tokenizer.pad_token is None:\n",
    "            self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token\n",
    "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "        params = count_parameters(self.model)\n",
    "        log.info(f\"[VLM] Model loaded. Total: {params['total']:,}, Trainable (pre-LoRA): {params['trainable']:,}\")\n",
    "\n",
    "    def _apply_lora(self):\n",
    "        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "        self.model = prepare_model_for_kbit_training(\n",
    "            self.model, use_gradient_checkpointing=self.vlm_cfg.gradient_checkpointing,\n",
    "        )\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.vlm_cfg.lora_r, lora_alpha=self.vlm_cfg.lora_alpha,\n",
    "            lora_dropout=self.vlm_cfg.lora_dropout,\n",
    "            target_modules=self.vlm_cfg.lora_target_modules,\n",
    "            bias=\"none\", task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        params = count_parameters(self.model)\n",
    "        log.info(f\"[VLM] LoRA applied. Trainable: {params['trainable']:,} ({params['trainable_pct']}%)\")\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "    def _load_datasets(self):\n",
    "        train_path = VLM_DATA_DIR / \"train.jsonl\"\n",
    "        val_path = VLM_DATA_DIR / \"val.jsonl\"\n",
    "        if not train_path.exists():\n",
    "            raise FileNotFoundError(f\"Training data not found at {train_path}. Run QA generator first!\")\n",
    "        train_dataset = GeoExtractVLMDataset(train_path, self.processor, max_length=self.vlm_cfg.max_seq_length)\n",
    "        val_dataset = None\n",
    "        if val_path.exists():\n",
    "            val_dataset = GeoExtractVLMDataset(val_path, self.processor, max_length=self.vlm_cfg.max_seq_length)\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def _setup_trainer(self, train_dataset, val_dataset):\n",
    "        from transformers import TrainingArguments, Trainer\n",
    "        resume_ckpt = None\n",
    "        if self.vlm_cfg.resume_from_checkpoint:\n",
    "            resume_ckpt = find_latest_checkpoint(Path(self.vlm_cfg.output_dir))\n",
    "            if resume_ckpt:\n",
    "                log.info(f\"[VLM] Will resume from: {resume_ckpt}\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.vlm_cfg.output_dir),\n",
    "            num_train_epochs=self.vlm_cfg.epochs,\n",
    "            per_device_train_batch_size=self.vlm_cfg.batch_size,\n",
    "            per_device_eval_batch_size=self.vlm_cfg.batch_size,\n",
    "            gradient_accumulation_steps=self.vlm_cfg.gradient_accumulation_steps,\n",
    "            learning_rate=self.vlm_cfg.learning_rate,\n",
    "            weight_decay=self.vlm_cfg.weight_decay,\n",
    "            warmup_ratio=self.vlm_cfg.warmup_ratio,\n",
    "            lr_scheduler_type=self.vlm_cfg.lr_scheduler_type,\n",
    "            fp16=self.vlm_cfg.fp16, bf16=self.vlm_cfg.bf16,\n",
    "            save_steps=self.vlm_cfg.save_steps, save_total_limit=3,\n",
    "            save_strategy=\"steps\",\n",
    "            eval_strategy=\"steps\" if val_dataset else \"no\",\n",
    "            eval_steps=self.vlm_cfg.eval_steps if val_dataset else None,\n",
    "            logging_steps=self.vlm_cfg.logging_steps, logging_first_step=True,\n",
    "            report_to=\"wandb\" if self.cfg.wandb.enabled else \"none\",\n",
    "            run_name=\"vlm-geoextract\",\n",
    "            gradient_checkpointing=self.vlm_cfg.gradient_checkpointing,\n",
    "            optim=\"paged_adamw_8bit\", max_grad_norm=1.0,\n",
    "            remove_unused_columns=False, dataloader_num_workers=2,\n",
    "            seed=self.cfg.data.seed,\n",
    "            load_best_model_at_end=True if val_dataset else False,\n",
    "            metric_for_best_model=\"eval_loss\" if val_dataset else None,\n",
    "        )\n",
    "        collator = ChatMLCollator(self.processor, max_length=self.vlm_cfg.max_seq_length)\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model, args=training_args,\n",
    "            train_dataset=train_dataset, eval_dataset=val_dataset,\n",
    "            data_collator=collator,\n",
    "        )\n",
    "        self._resume_checkpoint = resume_ckpt\n",
    "\n",
    "    def _run_training(self):\n",
    "        wandb_run = init_wandb(self.cfg, run_name=\"vlm-geoextract-lora\", tags=[\"vlm\", \"lora\", \"qwen2-vl\"])\n",
    "        try:\n",
    "            if self._resume_checkpoint:\n",
    "                log.info(f\"[VLM] Resuming from {self._resume_checkpoint}\")\n",
    "                self.trainer.train(resume_from_checkpoint=str(self._resume_checkpoint))\n",
    "            else:\n",
    "                log.info(\"[VLM] Starting training from scratch.\")\n",
    "                self.trainer.train()\n",
    "        except KeyboardInterrupt:\n",
    "            log.warning(\"[VLM] Training interrupted. Saving checkpoint...\")\n",
    "            self.trainer.save_model(str(self.vlm_cfg.output_dir / \"interrupted\"))\n",
    "        finally:\n",
    "            finish_wandb()\n",
    "\n",
    "    def _save_final(self) -> Path:\n",
    "        final_dir = Path(self.vlm_cfg.output_dir) / \"final_adapter\"\n",
    "        final_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.model.save_pretrained(str(final_dir))\n",
    "        self.processor.save_pretrained(str(final_dir))\n",
    "        log.info(f\"[VLM] Final adapter saved to {final_dir}\")\n",
    "        return final_dir\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.model is not None:\n",
    "            del self.model; self.model = None\n",
    "        if self.processor is not None:\n",
    "            del self.processor; self.processor = None\n",
    "        if self.trainer is not None:\n",
    "            del self.trainer; self.trainer = None\n",
    "        free_vram()\n",
    "        log.info(\"[VLM] Model unloaded, VRAM freed.\")\n",
    "\n",
    "print(\"✅ VLM trainer ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f72ef",
   "metadata": {},
   "source": [
    "## 8 · Agentic Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dda87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "\n",
    "\n",
    "class GeoExtractPipeline:\n",
    "    def __init__(self, cfg: InferenceConfig = CFG.inference, full_cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.full_cfg = full_cfg\n",
    "        self.yolo_model = None\n",
    "        self.vlm_model = None\n",
    "        self.vlm_processor = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        log_vram(\"before pipeline load\")\n",
    "        self._load_yolo()\n",
    "        self._load_vlm()\n",
    "        self._loaded = True\n",
    "        log_vram(\"after pipeline load\")\n",
    "        log.info(\"[Pipeline] ✓ Both models loaded and ready.\")\n",
    "\n",
    "    def _load_yolo(self):\n",
    "        from ultralytics import YOLO\n",
    "        weights_path = self.cfg.yolo_weights\n",
    "        if not weights_path.exists():\n",
    "            candidates = list(self.cfg.yolo_weights.parent.parent.rglob(\"best.pt\"))\n",
    "            if candidates:\n",
    "                weights_path = candidates[0]\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"YOLO weights not found at {self.cfg.yolo_weights}\")\n",
    "        self.yolo_model = YOLO(str(weights_path))\n",
    "        log.info(f\"[Pipeline] YOLO loaded from {weights_path}\")\n",
    "\n",
    "    def _load_vlm(self):\n",
    "        from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
    "        from peft import PeftModel\n",
    "        vlm_cfg = self.full_cfg.vlm\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            vlm_cfg.model_id, quantization_config=bnb_config,\n",
    "            device_map=\"auto\", trust_remote_code=True, torch_dtype=torch.float16,\n",
    "        )\n",
    "        adapter_dir = self.cfg.vlm_adapter_dir / \"final_adapter\"\n",
    "        if not adapter_dir.exists():\n",
    "            candidates = list(self.cfg.vlm_adapter_dir.rglob(\"adapter_config.json\"))\n",
    "            if candidates:\n",
    "                adapter_dir = candidates[0].parent\n",
    "            else:\n",
    "                log.warning(\"[Pipeline] No LoRA adapter found — using base model.\")\n",
    "                self.vlm_model = base_model\n",
    "                self.vlm_processor = AutoProcessor.from_pretrained(vlm_cfg.model_id, trust_remote_code=True)\n",
    "                return\n",
    "        self.vlm_model = PeftModel.from_pretrained(base_model, str(adapter_dir))\n",
    "        self.vlm_model.eval()\n",
    "        self.vlm_processor = AutoProcessor.from_pretrained(str(adapter_dir), trust_remote_code=True)\n",
    "        if self.vlm_processor.tokenizer.pad_token is None:\n",
    "            self.vlm_processor.tokenizer.pad_token = self.vlm_processor.tokenizer.eos_token\n",
    "        log.info(f\"[Pipeline] VLM loaded with adapter from {adapter_dir}\")\n",
    "\n",
    "    def analyze(self, image, question=None):\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        start_time = time.time()\n",
    "        if isinstance(image, (str, Path)):\n",
    "            image_path = str(image)\n",
    "            pil_image = PILImage.open(image_path).convert(\"RGB\")\n",
    "        else:\n",
    "            pil_image = image\n",
    "            image_path = \"uploaded_image\"\n",
    "\n",
    "        yolo_results = self._run_yolo(pil_image)\n",
    "        context = self._build_context(yolo_results)\n",
    "        if question is None:\n",
    "            question = (\n",
    "                \"Analyze this satellite image comprehensively. Assess the building \"\n",
    "                \"density, urban heat island risk, green space availability, \"\n",
    "                \"infrastructure stress, and provide urban planning recommendations.\"\n",
    "            )\n",
    "        vlm_response = self._run_vlm(pil_image, question, context)\n",
    "        return {\n",
    "            \"image\": image_path,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"processing_time_s\": round(time.time() - start_time, 2),\n",
    "            \"detection\": yolo_results,\n",
    "            \"context\": context,\n",
    "            \"analysis\": {\"question\": question, \"response\": vlm_response},\n",
    "            \"metadata\": {\n",
    "                \"yolo_model\": str(self.cfg.yolo_weights.name),\n",
    "                \"vlm_model\": self.full_cfg.vlm.model_id,\n",
    "                \"confidence_threshold\": self.cfg.confidence_threshold,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def batch_analyze(self, images, question=None):\n",
    "        results = []\n",
    "        for img in images:\n",
    "            try:\n",
    "                results.append(self.analyze(img, question))\n",
    "            except Exception as e:\n",
    "                results.append({\"image\": str(img), \"error\": str(e)})\n",
    "        return results\n",
    "\n",
    "    def _run_yolo(self, image):\n",
    "        results = self.yolo_model(\n",
    "            image, conf=self.cfg.confidence_threshold,\n",
    "            iou=self.cfg.iou_threshold, verbose=False,\n",
    "        )\n",
    "        detections = []\n",
    "        if results and len(results) > 0:\n",
    "            boxes = results[0].boxes\n",
    "            if boxes is not None:\n",
    "                for i in range(len(boxes)):\n",
    "                    detections.append({\n",
    "                        \"bbox\": boxes.xyxy[i].cpu().tolist(),\n",
    "                        \"confidence\": float(boxes.conf[i].cpu()),\n",
    "                        \"class\": int(boxes.cls[i].cpu()),\n",
    "                        \"class_name\": \"building\",\n",
    "                    })\n",
    "        return {\n",
    "            \"building_count\": len(detections),\n",
    "            \"detections\": detections,\n",
    "            \"avg_confidence\": round(np.mean([d[\"confidence\"] for d in detections]), 3) if detections else 0.0,\n",
    "        }\n",
    "\n",
    "    def _build_context(self, yolo_results):\n",
    "        count = yolo_results[\"building_count\"]\n",
    "        qa_cfg = self.full_cfg.qa\n",
    "        if count <= qa_cfg.sparse_max:\n",
    "            density = \"Sparse\"\n",
    "        elif count <= qa_cfg.moderate_max:\n",
    "            density = \"Moderate\"\n",
    "        elif count <= qa_cfg.dense_max:\n",
    "            density = \"Dense\"\n",
    "        else:\n",
    "            density = \"Urban Core\"\n",
    "        return {\n",
    "            \"building_count\": count, \"density_class\": density,\n",
    "            \"avg_detection_confidence\": yolo_results[\"avg_confidence\"],\n",
    "            \"context_prompt\": (\n",
    "                f\"The building detection model has identified {count} structures \"\n",
    "                f\"in this image with an average confidence of \"\n",
    "                f\"{yolo_results['avg_confidence']:.1%}. This area is classified \"\n",
    "                f\"as '{density}' density.\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def _run_vlm(self, image, question, context):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.full_cfg.qa.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": f\"Context from detection model: {context['context_prompt']}\\n\\nQuestion: {question}\"},\n",
    "            ]},\n",
    "        ]\n",
    "        try:\n",
    "            text = self.vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        except Exception:\n",
    "            text = (\n",
    "                f\"<|im_start|>system\\n{self.full_cfg.qa.system_prompt}<|im_end|>\\n\"\n",
    "                f\"<|im_start|>user\\n{context['context_prompt']}\\n{question}<|im_end|>\\n\"\n",
    "                f\"<|im_start|>assistant\\n\"\n",
    "            )\n",
    "        inputs = self.vlm_processor(\n",
    "            text=[text], images=[image], return_tensors=\"pt\", padding=True,\n",
    "        ).to(self.vlm_model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.vlm_model.generate(\n",
    "                **inputs, max_new_tokens=self.cfg.max_new_tokens,\n",
    "                do_sample=True, temperature=0.7, top_p=0.9, repetition_penalty=1.1,\n",
    "            )\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        return self.vlm_processor.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "    def chat(self, image, conversation_history, new_question):\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        if isinstance(image, (str, Path)):\n",
    "            pil_image = PILImage.open(str(image)).convert(\"RGB\")\n",
    "        else:\n",
    "            pil_image = image\n",
    "        if not conversation_history:\n",
    "            yolo_results = self._run_yolo(pil_image)\n",
    "            context = self._build_context(yolo_results)\n",
    "        else:\n",
    "            context = conversation_history[0].get(\"context\", {})\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": self.full_cfg.qa.system_prompt}]\n",
    "        for turn in conversation_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": turn.get(\"question\", \"\")})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": turn.get(\"response\", \"\")})\n",
    "        messages.append({\"role\": \"user\", \"content\": new_question})\n",
    "\n",
    "        text = self.vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.vlm_processor(\n",
    "            text=[text], images=[pil_image], return_tensors=\"pt\", padding=True,\n",
    "        ).to(self.vlm_model.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.vlm_model.generate(\n",
    "                **inputs, max_new_tokens=self.cfg.max_new_tokens,\n",
    "                do_sample=True, temperature=0.7,\n",
    "            )\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        response = self.vlm_processor.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True).strip()\n",
    "        conversation_history.append({\"question\": new_question, \"response\": response, \"context\": context})\n",
    "        return {\"response\": response, \"conversation_history\": conversation_history}\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.yolo_model is not None:\n",
    "            del self.yolo_model; self.yolo_model = None\n",
    "        if self.vlm_model is not None:\n",
    "            del self.vlm_model; self.vlm_model = None\n",
    "        if self.vlm_processor is not None:\n",
    "            del self.vlm_processor; self.vlm_processor = None\n",
    "        self._loaded = False\n",
    "        free_vram()\n",
    "        log.info(\"[Pipeline] All models unloaded.\")\n",
    "\n",
    "print(\"✅ Inference pipeline ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed9705",
   "metadata": {},
   "source": [
    "## 9 · Evaluation Module (Defense Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    ")\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class YOLOEvaluator:\n",
    "    def __init__(self, iou_threshold: float = 0.5):\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def evaluate(self, dataset_yaml: Path, weights_path: Path) -> Dict:\n",
    "        from ultralytics import YOLO\n",
    "        log.info(f\"[Eval-YOLO] Running validation with IoU={self.iou_threshold}...\")\n",
    "        model = YOLO(str(weights_path))\n",
    "        results = model.val(\n",
    "            data=str(dataset_yaml), iou=self.iou_threshold,\n",
    "            conf=0.25, verbose=True, plots=True,\n",
    "            save_dir=str(EVAL_DIR / \"yolo_eval\"),\n",
    "        )\n",
    "        metrics = {\n",
    "            \"precision\": float(results.box.mp) if hasattr(results.box, 'mp') else 0.0,\n",
    "            \"recall\": float(results.box.mr) if hasattr(results.box, 'mr') else 0.0,\n",
    "            \"mAP50\": float(results.box.map50) if hasattr(results.box, 'map50') else 0.0,\n",
    "            \"mAP50-95\": float(results.box.map) if hasattr(results.box, 'map') else 0.0,\n",
    "        }\n",
    "        p, r = metrics[\"precision\"], metrics[\"recall\"]\n",
    "        metrics[\"f1\"] = 2 * p * r / max(p + r, 1e-8)\n",
    "        log.info(f\"[Eval-YOLO] Results: {metrics}\")\n",
    "        out_path = EVAL_DIR / \"yolo_metrics.json\"\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(out_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_counting_accuracy(self, predictions, ground_truths) -> Dict:\n",
    "        predictions, ground_truths = np.array(predictions), np.array(ground_truths)\n",
    "        mae = float(np.mean(np.abs(predictions - ground_truths)))\n",
    "        rmse = float(np.sqrt(np.mean((predictions - ground_truths) ** 2)))\n",
    "        mape = float(np.mean(np.abs(predictions - ground_truths) / np.maximum(ground_truths, 1)) * 100)\n",
    "        metrics = {\n",
    "            \"mae\": mae, \"rmse\": rmse, \"mape_pct\": mape,\n",
    "            \"exact_match\": float(np.mean(predictions == ground_truths)),\n",
    "            \"within_5\": float(np.mean(np.abs(predictions - ground_truths) <= 5)),\n",
    "            \"within_10\": float(np.mean(np.abs(predictions - ground_truths) <= 10)),\n",
    "        }\n",
    "        log.info(f\"[Eval-YOLO] Counting accuracy: {metrics}\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class VLMEvaluator:\n",
    "    DENSITY_CLASSES = [\"Sparse\", \"Moderate\", \"Dense\", \"Urban Core\"]\n",
    "\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.DENSITY_CLASSES)}\n",
    "\n",
    "    def evaluate(self, gt_classes, pred_classes) -> Dict:\n",
    "        log.info(f\"[Eval-VLM] Evaluating {len(gt_classes)} samples...\")\n",
    "        gt_idx = [self.class_to_idx.get(c, -1) for c in gt_classes]\n",
    "        pred_idx = [self.class_to_idx.get(c, -1) for c in pred_classes]\n",
    "        valid = [(g, p) for g, p in zip(gt_idx, pred_idx) if g >= 0 and p >= 0]\n",
    "        if not valid:\n",
    "            log.error(\"[Eval-VLM] No valid predictions!\")\n",
    "            return {}\n",
    "        gt_valid = [v[0] for v in valid]\n",
    "        pred_valid = [v[1] for v in valid]\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": float(accuracy_score(gt_valid, pred_valid)),\n",
    "            \"f1_macro\": float(f1_score(gt_valid, pred_valid, average=\"macro\", zero_division=0)),\n",
    "            \"f1_weighted\": float(f1_score(gt_valid, pred_valid, average=\"weighted\", zero_division=0)),\n",
    "            \"precision_macro\": float(precision_score(gt_valid, pred_valid, average=\"macro\", zero_division=0)),\n",
    "            \"recall_macro\": float(recall_score(gt_valid, pred_valid, average=\"macro\", zero_division=0)),\n",
    "        }\n",
    "        report = classification_report(\n",
    "            gt_valid, pred_valid, target_names=self.DENSITY_CLASSES,\n",
    "            output_dict=True, zero_division=0,\n",
    "        )\n",
    "        metrics[\"per_class\"] = {\n",
    "            cls: {\"precision\": report[cls][\"precision\"], \"recall\": report[cls][\"recall\"],\n",
    "                  \"f1\": report[cls][\"f1-score\"], \"support\": report[cls][\"support\"]}\n",
    "            for cls in self.DENSITY_CLASSES if cls in report\n",
    "        }\n",
    "        cm = confusion_matrix(gt_valid, pred_valid, labels=list(range(len(self.DENSITY_CLASSES))))\n",
    "        metrics[\"confusion_matrix\"] = cm.tolist()\n",
    "\n",
    "        log.info(f\"[Eval-VLM] Accuracy: {metrics['accuracy']:.4f}, F1-macro: {metrics['f1_macro']:.4f}\")\n",
    "        print(\"\\n\" + classification_report(gt_valid, pred_valid, target_names=self.DENSITY_CLASSES, zero_division=0))\n",
    "\n",
    "        # Save confusion matrix plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        im = ax.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
    "               xticklabels=self.DENSITY_CLASSES, yticklabels=self.DENSITY_CLASSES,\n",
    "               title=\"Density Classification — Confusion Matrix\",\n",
    "               ylabel=\"Ground Truth\", xlabel=\"Predicted\")\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        thresh = cm.max() / 2.0\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], \"d\"), ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(str(EVAL_DIR / \"vlm_confusion_matrix.png\"), dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Save metrics\n",
    "        serializable = {k: v for k, v in metrics.items() if not isinstance(v, np.ndarray)}\n",
    "        with open(EVAL_DIR / \"vlm_metrics.json\", \"w\") as f:\n",
    "            json.dump(serializable, f, indent=2, default=str)\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_from_pipeline(self, pipeline, val_jsonl=None, max_samples=100):\n",
    "        if val_jsonl is None:\n",
    "            val_jsonl = VLM_DATA_DIR / \"val.jsonl\"\n",
    "        if not val_jsonl.exists():\n",
    "            log.error(f\"[Eval-VLM] Validation data not found: {val_jsonl}\")\n",
    "            return {}\n",
    "        with open(val_jsonl) as f:\n",
    "            val_data = [json.loads(l) for l in f.readlines()[:max_samples]]\n",
    "        gt_classes, pred_classes = [], []\n",
    "        for sample in tqdm(val_data, desc=\"VLM Evaluation\"):\n",
    "            gt_class = sample.get(\"density_class\", \"Unknown\")\n",
    "            image_path = sample.get(\"image\", \"\")\n",
    "            if not Path(image_path).exists():\n",
    "                continue\n",
    "            try:\n",
    "                result = pipeline.analyze(image_path)\n",
    "                pred_class = result.get(\"context\", {}).get(\"density_class\", \"Unknown\")\n",
    "                gt_classes.append(gt_class)\n",
    "                pred_classes.append(pred_class)\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[Eval-VLM] Failed on {image_path}: {e}\")\n",
    "        return self.evaluate(gt_classes, pred_classes)\n",
    "\n",
    "\n",
    "class GeoExtractEvaluator:\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.yolo_eval = YOLOEvaluator(cfg.evaluation.iou_threshold)\n",
    "        self.vlm_eval = VLMEvaluator(cfg)\n",
    "\n",
    "    def run_full_evaluation(self, dataset_yaml, yolo_weights, pipeline=None):\n",
    "        wandb_run = init_wandb(self.cfg, run_name=\"evaluation\", tags=[\"eval\", \"metrics\"])\n",
    "        all_metrics = {}\n",
    "\n",
    "        with Timer(\"YOLO Evaluation\"):\n",
    "            yolo_metrics = self.yolo_eval.evaluate(dataset_yaml, yolo_weights)\n",
    "            all_metrics[\"yolo\"] = yolo_metrics\n",
    "            if wandb_run:\n",
    "                import wandb\n",
    "                wandb.log({f\"eval/yolo_{k}\": v for k, v in yolo_metrics.items()})\n",
    "\n",
    "        if pipeline is not None:\n",
    "            with Timer(\"VLM Evaluation\"):\n",
    "                vlm_metrics = self.vlm_eval.evaluate_from_pipeline(pipeline)\n",
    "                all_metrics[\"vlm\"] = vlm_metrics\n",
    "                if wandb_run:\n",
    "                    import wandb\n",
    "                    log_m = {k: v for k, v in vlm_metrics.items() if isinstance(v, (int, float))}\n",
    "                    wandb.log({f\"eval/vlm_{k}\": v for k, v in log_m.items()})\n",
    "                    cm_path = EVAL_DIR / \"vlm_confusion_matrix.png\"\n",
    "                    if cm_path.exists():\n",
    "                        wandb.log({\"eval/confusion_matrix\": wandb.Image(str(cm_path))})\n",
    "\n",
    "        finish_wandb()\n",
    "        report_path = EVAL_DIR / \"full_evaluation_report.json\"\n",
    "        with open(report_path, \"w\") as f:\n",
    "            json.dump(all_metrics, f, indent=2, default=str)\n",
    "        log.info(f\"[Eval] ✓ Full report saved to {report_path}\")\n",
    "        self._print_summary(all_metrics)\n",
    "        return all_metrics\n",
    "\n",
    "    def _print_summary(self, metrics):\n",
    "        print(\"\\n\" + \"═\" * 60)\n",
    "        print(\"  GeoExtract v2 — EVALUATION SUMMARY\")\n",
    "        print(\"═\" * 60)\n",
    "        if \"yolo\" in metrics:\n",
    "            y = metrics[\"yolo\"]\n",
    "            print(f\"\\n  ┌── YOLO Building Detection ──┐\")\n",
    "            print(f\"  │ Precision:  {y.get('precision', 0):.4f}          │\")\n",
    "            print(f\"  │ Recall:     {y.get('recall', 0):.4f}          │\")\n",
    "            print(f\"  │ F1-Score:   {y.get('f1', 0):.4f}          │\")\n",
    "            print(f\"  │ mAP@50:     {y.get('mAP50', 0):.4f}          │\")\n",
    "            print(f\"  │ mAP@50-95:  {y.get('mAP50-95', 0):.4f}          │\")\n",
    "            print(f\"  └─────────────────────────────┘\")\n",
    "        if \"vlm\" in metrics:\n",
    "            v = metrics[\"vlm\"]\n",
    "            print(f\"\\n  ┌── VLM Density Classification ──┐\")\n",
    "            print(f\"  │ Accuracy:   {v.get('accuracy', 0):.4f}             │\")\n",
    "            print(f\"  │ F1 (macro): {v.get('f1_macro', 0):.4f}             │\")\n",
    "            print(f\"  │ F1 (wgt.):  {v.get('f1_weighted', 0):.4f}             │\")\n",
    "            print(f\"  │ Precision:  {v.get('precision_macro', 0):.4f}             │\")\n",
    "            print(f\"  │ Recall:     {v.get('recall_macro', 0):.4f}             │\")\n",
    "            print(f\"  └────────────────────────────────┘\")\n",
    "        print(\"═\" * 60)\n",
    "\n",
    "print(\"✅ Evaluation module ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3184e37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🚀 PIPELINE EXECUTION\n",
    "\n",
    "All code is loaded. Now run each step sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30797632",
   "metadata": {},
   "source": [
    "## Step 1 · Build YOLO Dataset from SpaceNet 7\n",
    "\n",
    "Parses GeoTIFF images + GeoJSON labels → YOLO bbox format with augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c05f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "builder = YOLODatasetBuilder(CFG)\n",
    "dataset_yaml = builder.build()\n",
    "\n",
    "# Print dataset statistics\n",
    "stats = builder.get_stats()\n",
    "print(\"\\n📊 Dataset Statistics:\")\n",
    "for split, s in stats.items():\n",
    "    print(f\"  {split}: {s['images']} images, {s['total_bboxes']} bboxes \"\n",
    "          f\"(avg {s['avg_bboxes_per_image']} per image)\")\n",
    "print(f\"\\n📁 Dataset YAML: {dataset_yaml}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793b15d",
   "metadata": {},
   "source": [
    "## Step 2 · Generate Synthetic QA Pairs for VLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_gen = SyntheticQAGenerator(CFG)\n",
    "qa_path = qa_gen.generate()\n",
    "\n",
    "# Print QA statistics\n",
    "qa_stats = qa_gen.get_stats()\n",
    "print(\"\\n📊 QA Statistics:\")\n",
    "for split, s in qa_stats.items():\n",
    "    print(f\"  {split}: {s['conversations']} conversations, {s['total_qa_turns']} QA turns\")\n",
    "    print(f\"    Density distribution: {s['density_distribution']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb1bc1",
   "metadata": {},
   "source": [
    "## Step 3 · Train YOLO Building Detector\n",
    "\n",
    "**Estimated time: ~1–2 hours on T4 GPU (50 epochs)**  \n",
    "Model auto-saves every 5 epochs. Safe to interrupt — will auto-resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a631748",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_trainer = YOLOTrainer(dataset_yaml, CFG)\n",
    "yolo_best_weights = yolo_trainer.train()\n",
    "print(f\"\\n✅ YOLO training complete. Best weights: {yolo_best_weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70ca1b",
   "metadata": {},
   "source": [
    "## Step 4 · Free YOLO VRAM → Train VLM with LoRA\n",
    "\n",
    "**Estimated time: ~2–4 hours on T4 GPU (3 epochs)**  \n",
    "Checkpoints every 500 steps. Auto-resume supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998095ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free YOLO from VRAM before loading VLM\n",
    "yolo_trainer.cleanup()\n",
    "del yolo_trainer\n",
    "free_vram()\n",
    "print(\"🧹 YOLO unloaded. VRAM is free for VLM.\")\n",
    "log_vram(\"before VLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2137ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_trainer = VLMTrainer(CFG)\n",
    "vlm_adapter_path = vlm_trainer.train()\n",
    "print(f\"\\n✅ VLM training complete. Adapter: {vlm_adapter_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free VLM trainer VRAM for inference\n",
    "vlm_trainer.cleanup()\n",
    "del vlm_trainer\n",
    "free_vram()\n",
    "print(\"🧹 VLM trainer unloaded.\")\n",
    "log_vram(\"before inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf6b00",
   "metadata": {},
   "source": [
    "## Step 5 · Run Agentic Inference Demo\n",
    "\n",
    "YOLO detects buildings → context injected → VLM reasons about the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the full pipeline (both models)\n",
    "pipeline = GeoExtractPipeline(CFG.inference, CFG)\n",
    "pipeline.load()\n",
    "\n",
    "# Find a sample image for demo\n",
    "sample_images = list(Path(YOLO_DATA_DIR / \"images\" / \"val\").glob(\"*.png\"))\n",
    "if sample_images:\n",
    "    demo_image = sample_images[0]\n",
    "    print(f\"\\n🖼️ Analyzing: {demo_image.name}\")\n",
    "    result = pipeline.analyze(demo_image)\n",
    "\n",
    "    print(f\"\\n🔍 Detection: {result['detection']['building_count']} buildings found\")\n",
    "    print(f\"📊 Density: {result['context']['density_class']}\")\n",
    "    print(f\"⏱️ Processing time: {result['processing_time_s']}s\")\n",
    "    print(f\"\\n💬 VLM Analysis:\\n{result['analysis']['response']}\")\n",
    "\n",
    "    # Save demo result\n",
    "    with open(OUTPUT_DIR / \"demo_result.json\", \"w\") as f:\n",
    "        json.dump(result, f, indent=2, default=str)\n",
    "    print(f\"\\n📁 Full result saved to {OUTPUT_DIR / 'demo_result.json'}\")\n",
    "else:\n",
    "    print(\"⚠️ No validation images found for demo. Run data pipeline first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6062a7",
   "metadata": {},
   "source": [
    "## Step 6 · Full Evaluation (Defense Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = GeoExtractEvaluator(CFG)\n",
    "yolo_weights = YOLO_CKPT_DIR / \"building_detector\" / \"weights\" / \"best.pt\"\n",
    "\n",
    "all_metrics = evaluator.run_full_evaluation(\n",
    "    dataset_yaml=dataset_yaml,\n",
    "    yolo_weights=yolo_weights,\n",
    "    pipeline=pipeline,\n",
    ")\n",
    "\n",
    "print(f\"\\n📁 Full report: {EVAL_DIR / 'full_evaluation_report.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final cleanup\n",
    "pipeline.cleanup()\n",
    "del pipeline\n",
    "free_vram()\n",
    "print(\"\\n🎉 GeoExtract v2 pipeline complete!\")\n",
    "print(f\"📂 All outputs saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8cc1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ⏱️ Estimated Training Times (NVIDIA T4 16 GB)\n",
    "\n",
    "| Step | Estimated Time | Notes |\n",
    "|------|---------------|-------|\n",
    "| **Data Processing** | 10–20 min | Depends on SpaceNet 7 subset size |\n",
    "| **QA Generation** | 5–15 min | GeoJSON parsing + template instantiation |\n",
    "| **YOLO Training** (50 epochs) | 1–2 hours | batch=16, img=640, YOLOv11-nano |\n",
    "| **VLM Training** (3 epochs) | 2–4 hours | batch=2, grad_accum=8, LoRA on 4-bit |\n",
    "| **Inference Demo** | 1–2 min | Single image end-to-end |\n",
    "| **Evaluation** | 15–30 min | YOLO val + VLM density classification |\n",
    "| **Total** | **~4–7 hours** | ✅ Within Kaggle 12-hour limit |\n",
    "\n",
    "### 💡 Tips to Speed Up\n",
    "- Set `CFG.data.max_samples = 500` for a faster debug run\n",
    "- Reduce YOLO epochs: `CFG.yolo.epochs = 25`\n",
    "- Reduce VLM epochs: `CFG.vlm.epochs = 1`\n",
    "- The pipeline auto-resumes from checkpoints if Kaggle session restarts"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
