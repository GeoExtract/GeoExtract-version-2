{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "299716f7",
   "metadata": {},
   "source": [
    "## 1 Â· Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f70750",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "!pip install -q \\\n",
    "    ultralytics>=8.3.0 \\\n",
    "    transformers>=4.45.0 \\\n",
    "    peft>=0.13.0 \\\n",
    "    bitsandbytes>=0.44.0 \\\n",
    "    accelerate>=1.0.0 \\\n",
    "    qwen-vl-utils \\\n",
    "    rasterio \\\n",
    "    geopandas \\\n",
    "    shapely \\\n",
    "    albumentations>=1.4.0 \\\n",
    "    wandb \\\n",
    "    scikit-learn \\\n",
    "    pyyaml \\\n",
    "    tqdm \\\n",
    "    matplotlib \\\n",
    "    pillow \\\n",
    "    opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802d470f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"âœ… All dependencies installed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85be305",
   "metadata": {},
   "source": [
    "## 2 Â· Configuration (Dynamic Paths, No Hardcoded Keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a8eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, time, json, random, shutil, logging, subprocess\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# â”€â”€â”€ Environment Detection â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IS_KAGGLE = os.path.exists(\"/kaggle/working\")\n",
    "\n",
    "def _resolve_root() -> Path:\n",
    "    if IS_KAGGLE:\n",
    "        return Path(\"/kaggle/working\")\n",
    "    return Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = _resolve_root()\n",
    "\n",
    "def _resolve_data_root() -> Path:\n",
    "    env = os.environ.get(\"SPACENET7_ROOT\")\n",
    "    if env:\n",
    "        return Path(env)\n",
    "    if IS_KAGGLE:\n",
    "        kaggle_input = Path(\"/kaggle/input\")\n",
    "        # Try exact name first\n",
    "        if (kaggle_input / \"spacenet7\").exists():\n",
    "            return kaggle_input / \"spacenet7\"\n",
    "        # Auto-discover: find ANY directory containing 'spacenet' in its name\n",
    "        candidates = sorted(\n",
    "            [d for d in kaggle_input.iterdir() if d.is_dir() and \"spacenet\" in d.name.lower()]\n",
    "        )\n",
    "        if candidates:\n",
    "            print(f\"[ğŸ“‚ Data] Auto-detected SpaceNet dataset: {candidates[0]}\")\n",
    "            return candidates[0]\n",
    "        # Fallback: list available datasets and raise helpful error\n",
    "        available = [d.name for d in kaggle_input.iterdir() if d.is_dir()]\n",
    "        raise FileNotFoundError(\n",
    "            f\"No SpaceNet dataset found under /kaggle/input/.\\n\"\n",
    "            f\"Available datasets: {available}\\n\"\n",
    "            f\"Please add SpaceNet 7 dataset to your Kaggle notebook, or set \\n\"\n",
    "            f\"SPACENET7_ROOT environment variable to the correct path.\"\n",
    "        )\n",
    "    return PROJECT_ROOT / \"data\"\n",
    "\n",
    "DATA_ROOT = _resolve_data_root()\n",
    "\n",
    "# â”€â”€â”€ Output Directories â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUTPUT_DIR     = PROJECT_ROOT / \"outputs\"\n",
    "YOLO_DATA_DIR  = OUTPUT_DIR / \"yolo_dataset\"\n",
    "VLM_DATA_DIR   = OUTPUT_DIR / \"vlm_dataset\"\n",
    "CHECKPOINT_DIR = OUTPUT_DIR / \"checkpoints\"\n",
    "YOLO_CKPT_DIR  = CHECKPOINT_DIR / \"yolo\"\n",
    "VLM_CKPT_DIR   = CHECKPOINT_DIR / \"vlm\"\n",
    "EVAL_DIR       = OUTPUT_DIR / \"evaluation\"\n",
    "EXPORT_DIR     = OUTPUT_DIR / \"export\"\n",
    "\n",
    "for _d in [YOLO_DATA_DIR, VLM_DATA_DIR, YOLO_CKPT_DIR, VLM_CKPT_DIR, EVAL_DIR, EXPORT_DIR]:\n",
    "    _d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#  KAGGLE HARD LIMITS â€” DO NOT CHANGE THESE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "KAGGLE_DISK_LIMIT_GB  = 20     # /kaggle/working has ~20 GB\n",
    "KAGGLE_VRAM_LIMIT_GB  = 16     # T4 has 15.1 GB usable\n",
    "DISK_SAFETY_MARGIN_GB = 3      # Reserve 3 GB for checkpoints/exports\n",
    "MAX_YOLO_SAMPLES      = 300    # Hard cap: ~300 images â‰ˆ 1.5 GB on disk\n",
    "MAX_VLM_SAMPLES       = 300    # Matching cap for QA conversations\n",
    "\n",
    "# â”€â”€â”€ W&B Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class WandbConfig:\n",
    "    project: str = \"GeoExtract-v2\"\n",
    "    entity: Optional[str] = None\n",
    "    enabled: bool = True\n",
    "    api_key: Optional[str] = field(default=None, repr=False)\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.api_key = os.environ.get(\"WANDB_API_KEY\", self.api_key)\n",
    "        self.entity = os.environ.get(\"WANDB_ENTITY\", self.entity)\n",
    "        if not self.api_key:\n",
    "            print(\"[âš  wandb] WANDB_API_KEY not set â€” logging disabled.\")\n",
    "            self.enabled = False\n",
    "\n",
    "# â”€â”€â”€ Dataset Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    root: Path = DATA_ROOT\n",
    "    images_subdir: str = \"train\"\n",
    "    geojson_subdir: str = \"train\"\n",
    "    image_size: int = 640\n",
    "    val_split: float = 0.15\n",
    "    seed: int = 42\n",
    "    max_samples: int = MAX_YOLO_SAMPLES   # ENFORCED cap for disk safety\n",
    "    augment: bool = True\n",
    "    rotation_limit: int = 20              # Lighter augmentation\n",
    "    color_jitter: float = 0.2\n",
    "    flip_prob: float = 0.5\n",
    "\n",
    "# â”€â”€â”€ YOLO Config (T4-tuned) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class YOLOConfig:\n",
    "    model_variant: str = \"yolo11n.pt\"\n",
    "    epochs: int = 30                      # 30 is enough for nano\n",
    "    batch_size: int = 8                   # Conservative for T4\n",
    "    image_size: int = 640\n",
    "    lr0: float = 1e-3\n",
    "    lrf: float = 0.01\n",
    "    patience: int = 7\n",
    "    save_period: int = 10                 # Less frequent saves = less disk\n",
    "    workers: int = 2\n",
    "    device: str = \"0\"\n",
    "    project: Path = YOLO_CKPT_DIR\n",
    "    name: str = \"building_detector\"\n",
    "    resume: bool = True\n",
    "\n",
    "# â”€â”€â”€ VLM Config (T4 OOM-proof) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class VLMConfig:\n",
    "    model_id: str = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "    load_in_4bit: bool = True\n",
    "    bnb_4bit_quant_type: str = \"nf4\"\n",
    "    bnb_4bit_compute_dtype: str = \"float16\"\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 32\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_target_modules: List[str] = field(\n",
    "        default_factory=lambda: [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
    "    )\n",
    "    epochs: int = 3\n",
    "    batch_size: int = 1                   # â† MUST be 1 for T4\n",
    "    gradient_accumulation_steps: int = 16 # â† Effective batch = 16\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_ratio: float = 0.1\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    max_seq_length: int = 512             # â† Slashed from 1024\n",
    "    save_steps: int = 200\n",
    "    logging_steps: int = 25\n",
    "    eval_steps: int = 200\n",
    "    output_dir: Path = VLM_CKPT_DIR\n",
    "    resume_from_checkpoint: bool = True\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False                    # T4 cannot do bf16\n",
    "    gradient_checkpointing: bool = True   # MANDATORY for T4\n",
    "    device: str = \"cuda:0\"\n",
    "    # T4 CANNOT use flash_attention_2 (needs SM>=80), force eager\n",
    "    attn_implementation: str = \"eager\"\n",
    "\n",
    "# â”€â”€â”€ QA Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class QAConfig:\n",
    "    sparse_max: int = 10\n",
    "    moderate_max: int = 30\n",
    "    dense_max: int = 50\n",
    "    min_turns: int = 2\n",
    "    max_turns: int = 3                    # Shorter convos = smaller tokens\n",
    "    system_prompt: str = (\n",
    "        \"You are GeoExtract, an expert urban planning AI that analyzes \"\n",
    "        \"satellite imagery. You provide detailed assessments of building \"\n",
    "        \"density, green space coverage, urban heat island risk, and \"\n",
    "        \"construction quality based on visual and spatial data.\"\n",
    "    )\n",
    "\n",
    "# â”€â”€â”€ Inference Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class InferenceConfig:\n",
    "    yolo_weights: Path = YOLO_CKPT_DIR / \"building_detector\" / \"weights\" / \"best.pt\"\n",
    "    vlm_adapter_dir: Path = VLM_CKPT_DIR\n",
    "    confidence_threshold: float = 0.25\n",
    "    iou_threshold: float = 0.45\n",
    "    max_new_tokens: int = 256             # Shorter generation = less VRAM\n",
    "    device: str = \"cuda:0\"\n",
    "\n",
    "# â”€â”€â”€ Evaluation Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class EvalConfig:\n",
    "    iou_threshold: float = 0.5\n",
    "    density_classes: List[str] = field(\n",
    "        default_factory=lambda: [\"Sparse\", \"Moderate\", \"Dense\", \"Urban Core\"]\n",
    "    )\n",
    "    output_dir: Path = EVAL_DIR\n",
    "    max_eval_samples: int = 50            # Cap evaluation to save time\n",
    "\n",
    "# â”€â”€â”€ Master Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@dataclass\n",
    "class GeoExtractConfig:\n",
    "    data: DataConfig = field(default_factory=DataConfig)\n",
    "    yolo: YOLOConfig = field(default_factory=YOLOConfig)\n",
    "    vlm: VLMConfig = field(default_factory=VLMConfig)\n",
    "    qa: QAConfig = field(default_factory=QAConfig)\n",
    "    inference: InferenceConfig = field(default_factory=InferenceConfig)\n",
    "    evaluation: EvalConfig = field(default_factory=EvalConfig)\n",
    "    wandb: WandbConfig = field(default_factory=WandbConfig)\n",
    "\n",
    "    def summary(self) -> str:\n",
    "        lines = [\n",
    "            \"â•\" * 55,\n",
    "            \"  GeoExtract v2 â€” Kaggle T4 Configuration\",\n",
    "            \"â•\" * 55,\n",
    "            f\"  Environment    : {'Kaggle' if IS_KAGGLE else 'Local'}\",\n",
    "            f\"  Project Root   : {PROJECT_ROOT}\",\n",
    "            f\"  Data Root      : {DATA_ROOT}\",\n",
    "            f\"  Output Dir     : {OUTPUT_DIR}\",\n",
    "            f\"  W&B Enabled    : {self.wandb.enabled}\",\n",
    "            \"â”€\" * 55,\n",
    "            f\"  Max Samples    : {self.data.max_samples} (disk cap)\",\n",
    "            f\"  YOLO model     : {self.yolo.model_variant}\",\n",
    "            f\"  YOLO epochs    : {self.yolo.epochs}\",\n",
    "            f\"  YOLO batch     : {self.yolo.batch_size}\",\n",
    "            \"â”€\" * 55,\n",
    "            f\"  VLM model      : {self.vlm.model_id}\",\n",
    "            f\"  VLM 4-bit      : {self.vlm.load_in_4bit}\",\n",
    "            f\"  Attention       : {self.vlm.attn_implementation} (T4 forced)\",\n",
    "            f\"  LoRA r/alpha   : {self.vlm.lora_r}/{self.vlm.lora_alpha}\",\n",
    "            f\"  VLM batch      : {self.vlm.batch_size} (OOM-safe)\",\n",
    "            f\"  VLM eff. batch : {self.vlm.batch_size * self.vlm.gradient_accumulation_steps}\",\n",
    "            f\"  Max seq length : {self.vlm.max_seq_length}\",\n",
    "            f\"  Grad checkpoint: {self.vlm.gradient_checkpointing}\",\n",
    "            \"â•\" * 55,\n",
    "        ]\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "CFG = GeoExtractConfig()\n",
    "print(CFG.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af47e4f",
   "metadata": {},
   "source": [
    "## 3 Â· Utility Helpers (VRAM, Logging, Checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Logging â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def setup_logger(name: str = \"geoextract\", level: int = logging.INFO) -> logging.Logger:\n",
    "    logger = logging.getLogger(name)\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        fmt = logging.Formatter(\n",
    "            \"[%(asctime)s] %(levelname)s â€” %(name)s â€” %(message)s\",\n",
    "            datefmt=\"%H:%M:%S\",\n",
    "        )\n",
    "        handler.setFormatter(fmt)\n",
    "        logger.addHandler(handler)\n",
    "    logger.setLevel(level)\n",
    "    return logger\n",
    "\n",
    "log = setup_logger()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Disk Monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_disk_usage_gb(path: str = \"/kaggle/working\") -> Dict:\n",
    "    \"\"\"Get disk usage for a path. Returns dict with used/free/total in GB.\"\"\"\n",
    "    try:\n",
    "        stat = shutil.disk_usage(path)\n",
    "        return {\n",
    "            \"total_gb\": round(stat.total / 1e9, 2),\n",
    "            \"used_gb\": round(stat.used / 1e9, 2),\n",
    "            \"free_gb\": round(stat.free / 1e9, 2),\n",
    "        }\n",
    "    except Exception:\n",
    "        return {\"total_gb\": 0, \"used_gb\": 0, \"free_gb\": 0}\n",
    "\n",
    "def check_disk_quota(required_gb: float = 1.0, path: str = \"/kaggle/working\") -> bool:\n",
    "    \"\"\"Check if there's enough disk space. Raises if critical.\"\"\"\n",
    "    usage = get_disk_usage_gb(path)\n",
    "    free = usage[\"free_gb\"]\n",
    "    if free < required_gb:\n",
    "        log.error(f\"[DISK] â›” Only {free:.1f} GB free, need {required_gb:.1f} GB!\")\n",
    "        log.error(f\"[DISK] Run cleanup: shutil.rmtree(path) on unused directories.\")\n",
    "        return False\n",
    "    log.info(f\"[DISK] âœ“ {free:.1f} GB free (need {required_gb:.1f} GB)\")\n",
    "    return True\n",
    "\n",
    "def log_disk(tag: str = \"\") -> None:\n",
    "    \"\"\"Log current disk usage.\"\"\"\n",
    "    if IS_KAGGLE:\n",
    "        d = get_disk_usage_gb(\"/kaggle/working\")\n",
    "        log.info(f\"[DISK {tag}] Used: {d['used_gb']} GB | Free: {d['free_gb']} GB | Total: {d['total_gb']} GB\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ VRAM Monitoring â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def get_vram_usage() -> dict:\n",
    "    if not torch.cuda.is_available():\n",
    "        return {\"allocated_gb\": 0, \"reserved_gb\": 0, \"total_gb\": 0}\n",
    "    return {\n",
    "        \"allocated_gb\": round(torch.cuda.memory_allocated() / 1e9, 2),\n",
    "        \"reserved_gb\": round(torch.cuda.memory_reserved() / 1e9, 2),\n",
    "        \"total_gb\": round(torch.cuda.get_device_properties(0).total_memory / 1e9, 2),\n",
    "    }\n",
    "\n",
    "def log_vram(tag: str = \"\") -> None:\n",
    "    v = get_vram_usage()\n",
    "    log.info(\n",
    "        f\"[VRAM {tag}] Allocated: {v['allocated_gb']} GB | \"\n",
    "        f\"Reserved: {v['reserved_gb']} GB | Total: {v['total_gb']} GB\"\n",
    "    )\n",
    "\n",
    "def free_vram() -> None:\n",
    "    \"\"\"Aggressively free GPU memory between training phases.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    log.info(\"[VRAM] Cache cleared.\")\n",
    "    log_vram(\"after cleanup\")\n",
    "\n",
    "def nuke_vram():\n",
    "    \"\"\"Nuclear option: delete ALL CUDA tensors from memory.\"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    gc.collect()\n",
    "    log.info(\"[VRAM] â˜¢ï¸ Nuclear cleanup done.\")\n",
    "    log_vram(\"after nuke\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Checkpoints â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def find_latest_checkpoint(ckpt_dir: Path, prefix: str = \"checkpoint-\") -> Optional[Path]:\n",
    "    if not ckpt_dir.exists():\n",
    "        return None\n",
    "    ckpts = sorted(\n",
    "        [d for d in ckpt_dir.iterdir() if d.is_dir() and d.name.startswith(prefix)],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else 0,\n",
    "    )\n",
    "    if ckpts:\n",
    "        log.info(f\"[Checkpoint] Found {len(ckpts)} checkpoints. Latest: {ckpts[-1].name}\")\n",
    "        return ckpts[-1]\n",
    "    return None\n",
    "\n",
    "def cleanup_old_checkpoints(ckpt_dir: Path, keep: int = 2, prefix: str = \"checkpoint-\"):\n",
    "    \"\"\"Delete all but the latest `keep` checkpoints to save disk space.\"\"\"\n",
    "    if not ckpt_dir.exists():\n",
    "        return\n",
    "    ckpts = sorted(\n",
    "        [d for d in ckpt_dir.iterdir() if d.is_dir() and d.name.startswith(prefix)],\n",
    "        key=lambda p: int(p.name.split(\"-\")[-1]) if p.name.split(\"-\")[-1].isdigit() else 0,\n",
    "    )\n",
    "    if len(ckpts) > keep:\n",
    "        for old in ckpts[:-keep]:\n",
    "            shutil.rmtree(old, ignore_errors=True)\n",
    "            log.info(f\"[Disk] ğŸ—‘ï¸ Deleted old checkpoint: {old.name}\")\n",
    "\n",
    "def count_parameters(model: torch.nn.Module) -> dict:\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return {\n",
    "        \"total\": total,\n",
    "        \"trainable\": trainable,\n",
    "        \"trainable_pct\": round(100 * trainable / total, 2) if total > 0 else 0,\n",
    "    }\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Timer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class Timer:\n",
    "    def __init__(self, label: str = \"Block\"):\n",
    "        self.label = label\n",
    "        self.start = 0.0\n",
    "        self.elapsed = 0.0\n",
    "    def __enter__(self):\n",
    "        self.start = time.time()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.elapsed = time.time() - self.start\n",
    "        log.info(f\"[Timer] {self.label} took {self.elapsed:.1f}s\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ W&B Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def init_wandb(config, run_name: str, tags: Optional[list] = None):\n",
    "    if not config.wandb.enabled:\n",
    "        log.warning(\"[wandb] Disabled â€” skipping init.\")\n",
    "        return None\n",
    "    try:\n",
    "        import wandb\n",
    "        os.environ[\"WANDB_API_KEY\"] = config.wandb.api_key\n",
    "        run = wandb.init(\n",
    "            project=config.wandb.project,\n",
    "            entity=config.wandb.entity,\n",
    "            name=run_name,\n",
    "            tags=tags or [],\n",
    "            config={\n",
    "                \"yolo_model\": config.yolo.model_variant,\n",
    "                \"vlm_model\": config.vlm.model_id,\n",
    "                \"lora_r\": config.vlm.lora_r,\n",
    "            },\n",
    "            reinit=True,\n",
    "        )\n",
    "        log.info(f\"[wandb] Run '{run_name}' initialized.\")\n",
    "        return run\n",
    "    except Exception as e:\n",
    "        log.error(f\"[wandb] Init failed: {e}. Continuing without logging.\")\n",
    "        return None\n",
    "\n",
    "def finish_wandb():\n",
    "    try:\n",
    "        import wandb\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Status Report â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "log_vram(\"initial\")\n",
    "log_disk(\"initial\")\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    cap = torch.cuda.get_device_capability(0)\n",
    "    print(f\"ğŸ–¥ï¸  GPU: {gpu_name} (SM {cap[0]}.{cap[1]})\")\n",
    "    if cap[0] < 8:\n",
    "        print(\"âš ï¸  T4 detected (SM 7.5) â†’ flash_attention_2 DISABLED, using eager attention\")\n",
    "print(\"âœ… Utilities ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bfdb9",
   "metadata": {},
   "source": [
    "## 4 Â· Data Pipeline (SpaceNet 7 â†’ YOLO Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b40a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import rasterio\n",
    "from rasterio.windows import Window\n",
    "from shapely.geometry import shape, box\n",
    "import geopandas as gpd\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class SpaceNet7Parser:\n",
    "    \"\"\"\n",
    "    Bulletproof SpaceNet 7 parser using recursive globbing.\n",
    "    Finds .tif + .geojson regardless of how deeply nested the dataset is.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg=CFG.data):\n",
    "        self.cfg = cfg\n",
    "        self.root = Path(cfg.root)\n",
    "        self.image_size = cfg.image_size\n",
    "        self._samples: List[Dict] = []\n",
    "\n",
    "    def discover(self) -> List[Dict]:\n",
    "        log.info(f\"[Data] Scanning {self.root} recursively for .tif files...\")\n",
    "\n",
    "        # â”€â”€ Recursive glob: find ALL .tif files anywhere under root â”€â”€\n",
    "        all_tifs = sorted(self.root.rglob(\"*.tif\"))\n",
    "        if not all_tifs:\n",
    "            # Try .tiff extension too\n",
    "            all_tifs = sorted(self.root.rglob(\"*.tiff\"))\n",
    "        if not all_tifs:\n",
    "            available = list(self.root.iterdir()) if self.root.exists() else []\n",
    "            raise FileNotFoundError(\n",
    "                f\"No .tif files found under {self.root}\\n\"\n",
    "                f\"Root contents: {[x.name for x in available[:20]]}\"\n",
    "            )\n",
    "\n",
    "        log.info(f\"[Data] Found {len(all_tifs)} .tif files via rglob.\")\n",
    "\n",
    "        samples = []\n",
    "        for tif_path in all_tifs:\n",
    "            # Skip non-image tifs (like mosaic/overview files that are huge)\n",
    "            if any(skip in tif_path.name.lower() for skip in [\"mosaic\", \"overview\", \"pan\"]):\n",
    "                continue\n",
    "\n",
    "            # Try to find a matching GeoJSON label\n",
    "            label_path = self._find_matching_geojson(tif_path)\n",
    "\n",
    "            # Determine AOI from directory structure\n",
    "            # Typical: .../train/AOI_XX_name/images/file.tif\n",
    "            aoi = self._extract_aoi(tif_path)\n",
    "\n",
    "            samples.append({\n",
    "                \"image_path\": tif_path,\n",
    "                \"label_path\": label_path,\n",
    "                \"aoi\": aoi,\n",
    "                \"timestamp\": tif_path.stem,\n",
    "                \"has_labels\": label_path is not None,\n",
    "            })\n",
    "\n",
    "        # â”€â”€ Enforce max_samples cap (CRITICAL for disk) â”€â”€\n",
    "        if self.cfg.max_samples and len(samples) > self.cfg.max_samples:\n",
    "            random.seed(self.cfg.seed)\n",
    "            samples = random.sample(samples, self.cfg.max_samples)\n",
    "            log.info(f\"[Data] âš ï¸ Capped to {self.cfg.max_samples} samples (disk safety)\")\n",
    "\n",
    "        self._samples = samples\n",
    "        labeled = sum(1 for s in samples if s[\"has_labels\"])\n",
    "        log.info(f\"[Data] Final: {len(samples)} samples, {labeled} with labels.\")\n",
    "        return samples\n",
    "\n",
    "    def _find_matching_geojson(self, tif_path: Path) -> Optional[Path]:\n",
    "        \"\"\"Search for a GeoJSON matching this .tif via multiple strategies.\"\"\"\n",
    "        stem = tif_path.stem\n",
    "\n",
    "        # Strategy 1: Look in a sibling 'labels' or 'labels_match' directory\n",
    "        parent = tif_path.parent\n",
    "        for labels_dir_name in [\"labels\", \"labels_match\", \"geojson\", \"annotations\"]:\n",
    "            labels_dir = parent.parent / labels_dir_name\n",
    "            if labels_dir.exists():\n",
    "                candidates = [\n",
    "                    labels_dir / f\"{stem}.geojson\",\n",
    "                    labels_dir / f\"{stem}_Buildings.geojson\",\n",
    "                    labels_dir / f\"Buildings_{stem}.geojson\",\n",
    "                ]\n",
    "                for c in candidates:\n",
    "                    if c.exists():\n",
    "                        return c\n",
    "                # Fuzzy match: stem appears in filename\n",
    "                for gj in labels_dir.glob(\"*.geojson\"):\n",
    "                    if stem in gj.stem or gj.stem in stem:\n",
    "                        return gj\n",
    "\n",
    "        # Strategy 2: GeoJSON in same directory as the .tif\n",
    "        same_dir = tif_path.with_suffix(\".geojson\")\n",
    "        if same_dir.exists():\n",
    "            return same_dir\n",
    "\n",
    "        # Strategy 3: rglob from AOI root for matching stem\n",
    "        aoi_root = parent.parent  # typically the AOI_XX directory\n",
    "        for gj in aoi_root.rglob(f\"*{stem}*.geojson\"):\n",
    "            return gj\n",
    "\n",
    "        return None\n",
    "\n",
    "    def _extract_aoi(self, tif_path: Path) -> str:\n",
    "        \"\"\"Extract AOI name from path. E.g., .../AOI_11_Rotterdam/... â†’ AOI_11_Rotterdam\"\"\"\n",
    "        for part in tif_path.parts:\n",
    "            if part.startswith(\"AOI_\") or part.startswith(\"aoi_\"):\n",
    "                return part\n",
    "        # Fallback: use grandparent dir name\n",
    "        return tif_path.parent.parent.name if len(tif_path.parts) >= 3 else \"unknown\"\n",
    "\n",
    "    def read_geotiff(self, path: Path) -> np.ndarray:\n",
    "        with rasterio.open(path) as src:\n",
    "            img = src.read()\n",
    "        img = np.transpose(img, (1, 2, 0))\n",
    "        if img.shape[2] > 3:\n",
    "            img = img[:, :, :3]\n",
    "        if img.dtype != np.uint8:\n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        return img\n",
    "\n",
    "    def read_geojson(self, path: Path) -> List[Dict]:\n",
    "        if path is None or not path.exists():\n",
    "            return []\n",
    "        try:\n",
    "            gdf = gpd.read_file(path)\n",
    "            buildings = []\n",
    "            for _, row in gdf.iterrows():\n",
    "                geom = row.geometry\n",
    "                if geom is not None and geom.is_valid:\n",
    "                    buildings.append({\n",
    "                        \"geometry\": geom,\n",
    "                        \"bounds\": geom.bounds,\n",
    "                        \"area\": geom.area,\n",
    "                        \"properties\": {k: v for k, v in row.items() if k != \"geometry\"},\n",
    "                    })\n",
    "            return buildings\n",
    "        except Exception as e:\n",
    "            log.warning(f\"[Data] Failed to read {path}: {e}\")\n",
    "            return []\n",
    "\n",
    "    def get_image_metadata(self, path: Path) -> Dict:\n",
    "        with rasterio.open(path) as src:\n",
    "            return {\n",
    "                \"crs\": str(src.crs), \"transform\": src.transform,\n",
    "                \"bounds\": src.bounds, \"width\": src.width, \"height\": src.height,\n",
    "            }\n",
    "\n",
    "\n",
    "class YOLOFormatConverter:\n",
    "    CLASS_BUILDING = 0\n",
    "\n",
    "    def __init__(self, image_size: int = 640):\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def polygon_to_yolo_bbox(self, geometry, img_width, img_height, geo_transform=None):\n",
    "        minx, miny, maxx, maxy = geometry.bounds\n",
    "        if geo_transform is not None:\n",
    "            from rasterio.transform import rowcol\n",
    "            row_min, col_min = rowcol(geo_transform, minx, maxy)\n",
    "            row_max, col_max = rowcol(geo_transform, maxx, miny)\n",
    "            px_xmin = max(0, min(col_min, col_max))\n",
    "            px_ymin = max(0, min(row_min, row_max))\n",
    "            px_xmax = min(img_width, max(col_min, col_max))\n",
    "            px_ymax = min(img_height, max(row_min, row_max))\n",
    "        else:\n",
    "            px_xmin, px_ymin = max(0, minx), max(0, miny)\n",
    "            px_xmax, px_ymax = min(img_width, maxx), min(img_height, maxy)\n",
    "\n",
    "        bw, bh = px_xmax - px_xmin, px_ymax - px_ymin\n",
    "        if bw <= 2 or bh <= 2:\n",
    "            return None\n",
    "        x_center = np.clip((px_xmin + bw / 2) / img_width, 0.0, 1.0)\n",
    "        y_center = np.clip((px_ymin + bh / 2) / img_height, 0.0, 1.0)\n",
    "        w_norm   = np.clip(bw / img_width, 0.0, 1.0)\n",
    "        h_norm   = np.clip(bh / img_height, 0.0, 1.0)\n",
    "        return (self.CLASS_BUILDING, x_center, y_center, w_norm, h_norm)\n",
    "\n",
    "    def convert_sample(self, image_path, buildings, geo_transform=None):\n",
    "        with rasterio.open(image_path) as src:\n",
    "            img_w, img_h = src.width, src.height\n",
    "            if geo_transform is None:\n",
    "                geo_transform = src.transform\n",
    "        bboxes = []\n",
    "        for bld in buildings:\n",
    "            bbox = self.polygon_to_yolo_bbox(bld[\"geometry\"], img_w, img_h, geo_transform)\n",
    "            if bbox is not None:\n",
    "                bboxes.append(bbox)\n",
    "        return bboxes\n",
    "\n",
    "\n",
    "def build_augmentation_pipeline(cfg=CFG.data) -> A.Compose:\n",
    "    transforms = []\n",
    "    if cfg.augment:\n",
    "        transforms.extend([\n",
    "            A.HorizontalFlip(p=cfg.flip_prob),\n",
    "            A.VerticalFlip(p=cfg.flip_prob * 0.5),\n",
    "            A.RandomRotate90(p=0.3),\n",
    "            A.Rotate(limit=cfg.rotation_limit, p=0.4, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.ColorJitter(\n",
    "                brightness=cfg.color_jitter, contrast=cfg.color_jitter,\n",
    "                saturation=cfg.color_jitter * 0.5, hue=cfg.color_jitter * 0.2, p=0.5,\n",
    "            ),\n",
    "        ])\n",
    "    transforms.append(A.Resize(cfg.image_size, cfg.image_size))\n",
    "    return A.Compose(\n",
    "        transforms,\n",
    "        bbox_params=A.BboxParams(format=\"yolo\", label_fields=[\"class_labels\"], min_visibility=0.3),\n",
    "    )\n",
    "\n",
    "\n",
    "class YOLODatasetBuilder:\n",
    "    \"\"\"\n",
    "    Disk-safe YOLO dataset builder.\n",
    "    - Enforces max_samples cap to avoid blowing 20GB disk limit\n",
    "    - Saves as JPEG (not PNG) â†’ ~5x smaller files\n",
    "    - Monitors disk usage throughout build\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.parser = SpaceNet7Parser(cfg.data)\n",
    "        self.converter = YOLOFormatConverter(cfg.data.image_size)\n",
    "        self.augmenter = build_augmentation_pipeline(cfg.data)\n",
    "        self.output_dir = YOLO_DATA_DIR\n",
    "\n",
    "    def build(self) -> Path:\n",
    "        with Timer(\"YOLO Dataset Build\"):\n",
    "            # â”€â”€ Pre-flight disk check â”€â”€\n",
    "            if IS_KAGGLE:\n",
    "                check_disk_quota(required_gb=DISK_SAFETY_MARGIN_GB)\n",
    "                log_disk(\"before build\")\n",
    "\n",
    "            samples = self.parser.discover()\n",
    "            labeled_samples = [s for s in samples if s[\"has_labels\"]]\n",
    "            if not labeled_samples:\n",
    "                raise ValueError(f\"No labeled samples in {self.cfg.data.root}\")\n",
    "\n",
    "            random.seed(self.cfg.data.seed)\n",
    "            random.shuffle(labeled_samples)\n",
    "            split_idx = int(len(labeled_samples) * (1 - self.cfg.data.val_split))\n",
    "            train_samples = labeled_samples[:split_idx]\n",
    "            val_samples = labeled_samples[split_idx:]\n",
    "            log.info(f\"[Data] Split: {len(train_samples)} train, {len(val_samples)} val\")\n",
    "\n",
    "            for split in [\"train\", \"val\"]:\n",
    "                (self.output_dir / \"images\" / split).mkdir(parents=True, exist_ok=True)\n",
    "                (self.output_dir / \"labels\" / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            self._process_split(train_samples, \"train\", augment=True)\n",
    "            self._process_split(val_samples, \"val\", augment=False)\n",
    "            yaml_path = self._write_dataset_yaml()\n",
    "\n",
    "            if IS_KAGGLE:\n",
    "                log_disk(\"after build\")\n",
    "            log.info(f\"[Data] âœ“ YOLO dataset ready at {self.output_dir}\")\n",
    "            return yaml_path\n",
    "\n",
    "    def _process_split(self, samples, split, augment):\n",
    "        img_dir = self.output_dir / \"images\" / split\n",
    "        lbl_dir = self.output_dir / \"labels\" / split\n",
    "        skipped = 0\n",
    "        for sample in tqdm(samples, desc=f\"Processing {split}\"):\n",
    "            try:\n",
    "                # Disk check every 50 samples\n",
    "                if IS_KAGGLE and skipped % 50 == 0:\n",
    "                    usage = get_disk_usage_gb(\"/kaggle/working\")\n",
    "                    if usage[\"free_gb\"] < DISK_SAFETY_MARGIN_GB:\n",
    "                        log.warning(f\"[Data] â›” Stopping {split}: only {usage['free_gb']:.1f} GB free!\")\n",
    "                        break\n",
    "\n",
    "                img = self.parser.read_geotiff(sample[\"image_path\"])\n",
    "                buildings = self.parser.read_geojson(sample[\"label_path\"])\n",
    "                meta = self.parser.get_image_metadata(sample[\"image_path\"])\n",
    "                bboxes = self.converter.convert_sample(\n",
    "                    sample[\"image_path\"], buildings, meta.get(\"transform\")\n",
    "                )\n",
    "                if not bboxes:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "                yolo_bboxes = [(b[1], b[2], b[3], b[4]) for b in bboxes]\n",
    "                class_labels = [b[0] for b in bboxes]\n",
    "\n",
    "                if augment and self.cfg.data.augment:\n",
    "                    try:\n",
    "                        augmented = self.augmenter(\n",
    "                            image=img, bboxes=yolo_bboxes, class_labels=class_labels,\n",
    "                        )\n",
    "                        img = augmented[\"image\"]\n",
    "                        yolo_bboxes = augmented[\"bboxes\"]\n",
    "                        class_labels = augmented[\"class_labels\"]\n",
    "                    except Exception:\n",
    "                        img = cv2.resize(img, (self.cfg.data.image_size, self.cfg.data.image_size))\n",
    "                else:\n",
    "                    img = cv2.resize(img, (self.cfg.data.image_size, self.cfg.data.image_size))\n",
    "\n",
    "                if not yolo_bboxes:\n",
    "                    skipped += 1\n",
    "                    continue\n",
    "\n",
    "                stem = f\"{sample['aoi']}_{sample['timestamp']}\"\n",
    "                # JPEG instead of PNG â†’ ~5x less disk usage\n",
    "                cv2.imwrite(\n",
    "                    str(img_dir / f\"{stem}.jpg\"),\n",
    "                    cv2.cvtColor(img, cv2.COLOR_RGB2BGR),\n",
    "                    [cv2.IMWRITE_JPEG_QUALITY, 90],\n",
    "                )\n",
    "                with open(lbl_dir / f\"{stem}.txt\", \"w\") as f:\n",
    "                    for cls_id, bbox in zip(class_labels, yolo_bboxes):\n",
    "                        f.write(f\"{cls_id} {bbox[0]:.6f} {bbox[1]:.6f} {bbox[2]:.6f} {bbox[3]:.6f}\\n\")\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[Data] Failed {sample['image_path'].name}: {e}\")\n",
    "                skipped += 1\n",
    "\n",
    "        log.info(f\"[Data] {split}: wrote {len(samples) - skipped} images, skipped {skipped}\")\n",
    "\n",
    "    def _write_dataset_yaml(self) -> Path:\n",
    "        import yaml\n",
    "        yaml_content = {\n",
    "            \"path\": str(self.output_dir), \"train\": \"images/train\",\n",
    "            \"val\": \"images/val\", \"nc\": 1, \"names\": [\"building\"],\n",
    "        }\n",
    "        yaml_path = self.output_dir / \"dataset.yaml\"\n",
    "        with open(yaml_path, \"w\") as f:\n",
    "            yaml.dump(yaml_content, f, default_flow_style=False)\n",
    "        return yaml_path\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        stats = {}\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            img_dir = self.output_dir / \"images\" / split\n",
    "            lbl_dir = self.output_dir / \"labels\" / split\n",
    "            n_images = len(list(img_dir.glob(\"*.jpg\"))) + len(list(img_dir.glob(\"*.png\")))\n",
    "            total_boxes = 0\n",
    "            for lbl_file in lbl_dir.glob(\"*.txt\"):\n",
    "                with open(lbl_file) as f:\n",
    "                    total_boxes += len(f.readlines())\n",
    "            stats[split] = {\n",
    "                \"images\": n_images, \"total_bboxes\": total_boxes,\n",
    "                \"avg_bboxes_per_image\": round(total_boxes / max(n_images, 1), 1),\n",
    "            }\n",
    "        return stats\n",
    "\n",
    "print(\"âœ… Data pipeline ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba067da6",
   "metadata": {},
   "source": [
    "## 5 Â· Synthetic QA Generator (ChatML Conversations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3136cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DensityClassifier:\n",
    "    def __init__(self, cfg=CFG.qa):\n",
    "        self.cfg = cfg\n",
    "\n",
    "    def classify(self, building_count: int, image_area_m2: Optional[float] = None) -> Dict:\n",
    "        if building_count <= self.cfg.sparse_max:\n",
    "            density_class, density_desc = \"Sparse\", \"Low-density suburban or rural area\"\n",
    "            heat_risk, green_space = \"Low\", \"Abundant â€” large open and vegetated areas visible\"\n",
    "            livability = \"High â€” spacious residential environment\"\n",
    "            construction_intensity = \"Minimal\"\n",
    "        elif building_count <= self.cfg.moderate_max:\n",
    "            density_class, density_desc = \"Moderate\", \"Moderate suburban density with mixed land use\"\n",
    "            heat_risk, green_space = \"Moderate\", \"Moderate â€” some green patches between structures\"\n",
    "            livability = \"Good â€” balanced density with accessible open areas\"\n",
    "            construction_intensity = \"Active â€” ongoing development likely\"\n",
    "        elif building_count <= self.cfg.dense_max:\n",
    "            density_class, density_desc = \"Dense\", \"High-density urban area with tightly packed structures\"\n",
    "            heat_risk, green_space = \"High\", \"Limited â€” minimal vegetation corridors\"\n",
    "            livability = \"Moderate â€” constrained but functional residential zones\"\n",
    "            construction_intensity = \"High â€” significant built-up coverage\"\n",
    "        else:\n",
    "            density_class, density_desc = \"Urban Core\", \"Hyper-dense urban core with maximum building coverage\"\n",
    "            heat_risk, green_space = \"Very High\", \"Severely depleted â€” critical lack of vegetation\"\n",
    "            livability = \"Low â€” crowded environment with limited open space\"\n",
    "            construction_intensity = \"Maximum â€” near-complete land coverage\"\n",
    "\n",
    "        buildings_per_hectare = building_count / max(1, (image_area_m2 or 409600) / 10000)\n",
    "        return {\n",
    "            \"building_count\": building_count, \"density_class\": density_class,\n",
    "            \"density_description\": density_desc, \"heat_island_risk\": heat_risk,\n",
    "            \"green_space_assessment\": green_space, \"livability_rating\": livability,\n",
    "            \"construction_intensity\": construction_intensity,\n",
    "            \"buildings_per_hectare\": round(buildings_per_hectare, 1),\n",
    "        }\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€ QA Templates â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def _q_density_analysis(info):\n",
    "    return (\n",
    "        \"Analyze the building density in this satellite image. What type of urban zone does this represent?\",\n",
    "        f\"This area shows a **{info['density_class']}** density pattern with approximately \"\n",
    "        f\"{info['building_count']} buildings detected. {info['density_description']}. \"\n",
    "        f\"The estimated building density is {info['buildings_per_hectare']} buildings per hectare.\"\n",
    "    )\n",
    "\n",
    "def _q_heat_island(info):\n",
    "    return (\n",
    "        \"What is the urban heat island risk for this area based on the visible building coverage?\",\n",
    "        f\"The urban heat island risk is **{info['heat_island_risk']}**. With {info['building_count']} \"\n",
    "        f\"structures detected, the built-up area significantly \"\n",
    "        f\"{'increases' if info['building_count'] > 30 else 'modestly affects'} surface temperature \"\n",
    "        f\"relative to surrounding undeveloped land. Construction intensity: {info['construction_intensity']}.\"\n",
    "    )\n",
    "\n",
    "def _q_green_space(info):\n",
    "    return (\n",
    "        \"Assess the green space availability and environmental health of this area.\",\n",
    "        f\"Green space assessment: {info['green_space_assessment']}. In this \"\n",
    "        f\"{info['density_class'].lower()}-density zone, vegetation coverage \"\n",
    "        f\"{'provides adequate cooling and biodiversity corridors' if info['building_count'] <= 20 else 'is insufficient for effective microclimate regulation'}. \"\n",
    "        f\"Recommendation: {'Maintain current balance' if info['building_count'] <= 20 else 'Prioritize urban greening initiatives and rooftop gardens'}.\"\n",
    "    )\n",
    "\n",
    "def _q_livability(info):\n",
    "    return (\n",
    "        \"Rate the residential livability of this zone. Would you recommend it for new housing development?\",\n",
    "        f\"Livability rating: {info['livability_rating']}. With a {info['density_class'].lower()} \"\n",
    "        f\"building density of {info['buildings_per_hectare']} structures per hectare, \"\n",
    "        f\"{'this area has capacity for additional development while maintaining quality of life' if info['building_count'] <= 25 else 'further development should be carefully planned to avoid overcrowding and infrastructure strain'}.\"\n",
    "    )\n",
    "\n",
    "def _q_construction_trend(info):\n",
    "    return (\n",
    "        \"What can you tell about the construction activity and urban growth pattern in this area?\",\n",
    "        f\"Construction intensity: {info['construction_intensity']}. The {info['building_count']} \"\n",
    "        f\"detected structures suggest \"\n",
    "        f\"{'an early-stage development area with significant growth potential' if info['building_count'] <= 15 else 'a mature built environment' if info['building_count'] > 40 else 'an actively developing zone in mid-growth phase'}. \"\n",
    "        f\"The spatial distribution indicates \"\n",
    "        f\"{'organic/informal growth patterns' if info['building_count'] > 45 else 'planned development with identifiable street grids'}.\"\n",
    "    )\n",
    "\n",
    "def _q_infrastructure(info):\n",
    "    return (\n",
    "        \"Based on the building density and layout, what infrastructure challenges might this area face?\",\n",
    "        f\"With {info['building_count']} buildings in this tile, key infrastructure considerations include: \"\n",
    "        f\"{'Water and sewage â€” adequate capacity likely available' if info['building_count'] <= 20 else 'Water and sewage â€” systems may be at or near capacity'}. \"\n",
    "        f\"{'Road network â€” sufficient for current density' if info['building_count'] <= 30 else 'Road network â€” congestion risk is elevated'}. \"\n",
    "        f\"{'Power grid â€” standard residential load' if info['building_count'] <= 25 else 'Power grid â€” peak demand management needed'}. \"\n",
    "        f\"Overall infrastructure stress: {'Low' if info['building_count'] <= 15 else 'Moderate' if info['building_count'] <= 35 else 'High' if info['building_count'] <= 50 else 'Critical'}.\"\n",
    "    )\n",
    "\n",
    "def _q_planning_recommendation(info):\n",
    "    return (\n",
    "        \"If you were an urban planner, what would you recommend for this area's future development?\",\n",
    "        f\"For this {info['density_class'].lower()}-density area ({info['building_count']} structures), \"\n",
    "        f\"I recommend: \"\n",
    "        f\"{'1) Controlled expansion with green buffer zones, 2) Mixed-use zoning, 3) Investment in public transit corridors' if info['building_count'] <= 25 else '1) Densification limits, 2) Mandatory green space ratios, 3) Stormwater management infrastructure upgrades' if info['building_count'] <= 45 else '1) Construction moratorium until infrastructure catches up, 2) Energy efficiency retrofitting, 3) Creating pocket parks to combat heat island effects'}.\"\n",
    "    )\n",
    "\n",
    "def _q_environmental_impact(info):\n",
    "    return (\n",
    "        \"What is the environmental footprint of this built-up area? Discuss carbon implications and ecological connectivity.\",\n",
    "        f\"Environmental analysis for {info['density_class']} zone ({info['building_count']} structures): \"\n",
    "        f\"Carbon footprint: {'Low â€” minimal impervious surface' if info['building_count'] <= 10 else 'Moderate â€” significant impervious surfaces' if info['building_count'] <= 30 else 'High â€” extensive land sealing'}. \"\n",
    "        f\"Ecological connectivity: {'Intact â€” wildlife corridors preserved' if info['building_count'] <= 15 else 'Fragmented â€” habitat patches isolated' if info['building_count'] <= 40 else 'Severely disrupted â€” near-complete habitat loss'}. \"\n",
    "        f\"Stormwater: {'Natural infiltration adequate' if info['building_count'] <= 20 else 'Engineered drainage required to prevent flooding'}.\"\n",
    "    )\n",
    "\n",
    "\n",
    "QA_TEMPLATES = [\n",
    "    _q_density_analysis, _q_heat_island, _q_green_space, _q_livability,\n",
    "    _q_construction_trend, _q_infrastructure, _q_planning_recommendation,\n",
    "    _q_environmental_impact,\n",
    "]\n",
    "\n",
    "\n",
    "class SyntheticQAGenerator:\n",
    "    \"\"\"\n",
    "    Disk-safe QA generator.\n",
    "    - Streams JSONL writes (no giant list in memory)\n",
    "    - Enforces MAX_VLM_SAMPLES cap\n",
    "    - Shorter conversations (max_turns=3)\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.qa_cfg = cfg.qa\n",
    "        self.parser = SpaceNet7Parser(cfg.data)\n",
    "        self.classifier = DensityClassifier(cfg.qa)\n",
    "        self.output_dir = VLM_DATA_DIR\n",
    "\n",
    "    def generate(self) -> Path:\n",
    "        with Timer(\"VLM QA Generation\"):\n",
    "            samples = self.parser.discover()\n",
    "            labeled = [s for s in samples if s[\"has_labels\"]]\n",
    "\n",
    "            # Cap to prevent disk bloat\n",
    "            if len(labeled) > MAX_VLM_SAMPLES:\n",
    "                random.seed(self.cfg.data.seed)\n",
    "                labeled = random.sample(labeled, MAX_VLM_SAMPLES)\n",
    "                log.info(f\"[QA] Capped to {MAX_VLM_SAMPLES} samples for disk safety.\")\n",
    "\n",
    "            conversations = []\n",
    "            for sample in tqdm(labeled, desc=\"Generating QA pairs\"):\n",
    "                try:\n",
    "                    conv = self._generate_conversation(sample)\n",
    "                    if conv:\n",
    "                        conversations.append(conv)\n",
    "                except Exception as e:\n",
    "                    log.warning(f\"[QA] Failed for {sample['image_path'].name}: {e}\")\n",
    "\n",
    "            random.seed(self.cfg.data.seed)\n",
    "            random.shuffle(conversations)\n",
    "            split_idx = int(len(conversations) * (1 - self.cfg.data.val_split))\n",
    "\n",
    "            # Stream writes to avoid holding everything in memory\n",
    "            train_path = self._save_jsonl(conversations[:split_idx], \"train.jsonl\")\n",
    "            self._save_jsonl(conversations[split_idx:], \"val.jsonl\")\n",
    "            log.info(f\"[QA] Generated {split_idx} train, {len(conversations) - split_idx} val conversations.\")\n",
    "\n",
    "            if IS_KAGGLE:\n",
    "                log_disk(\"after QA generation\")\n",
    "            return train_path\n",
    "\n",
    "    def _generate_conversation(self, sample):\n",
    "        buildings = self.parser.read_geojson(sample[\"label_path\"])\n",
    "        building_count = len(buildings)\n",
    "        total_area = sum(b.get(\"area\", 0) for b in buildings) if buildings else None\n",
    "        density_info = self.classifier.classify(building_count, total_area)\n",
    "\n",
    "        n_turns = random.randint(self.qa_cfg.min_turns, self.qa_cfg.max_turns)\n",
    "        selected_templates = random.sample(QA_TEMPLATES, min(n_turns, len(QA_TEMPLATES)))\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": self.qa_cfg.system_prompt}]\n",
    "        for template_fn in selected_templates:\n",
    "            question, answer = template_fn(density_info)\n",
    "            messages.append({\"role\": \"user\", \"content\": question})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": answer})\n",
    "\n",
    "        return {\n",
    "            \"id\": f\"{sample['aoi']}_{sample['timestamp']}\",\n",
    "            \"image\": str(sample[\"image_path\"]),\n",
    "            \"building_count\": building_count,\n",
    "            \"density_class\": density_info[\"density_class\"],\n",
    "            \"messages\": messages,\n",
    "        }\n",
    "\n",
    "    def _save_jsonl(self, conversations, filename):\n",
    "        path = self.output_dir / filename\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            for conv in conversations:\n",
    "                f.write(json.dumps(conv, default=str) + \"\\n\")\n",
    "        log.info(f\"[QA] Saved {len(conversations)} conversations to {path}\")\n",
    "        return path\n",
    "\n",
    "    def get_stats(self) -> Dict:\n",
    "        stats = {}\n",
    "        for split in [\"train\", \"val\"]:\n",
    "            path = self.output_dir / f\"{split}.jsonl\"\n",
    "            if path.exists():\n",
    "                with open(path) as f:\n",
    "                    convs = [json.loads(l) for l in f.readlines()]\n",
    "                density_dist = {}\n",
    "                total_turns = 0\n",
    "                for c in convs:\n",
    "                    dc = c.get(\"density_class\", \"Unknown\")\n",
    "                    density_dist[dc] = density_dist.get(dc, 0) + 1\n",
    "                    total_turns += len([m for m in c[\"messages\"] if m[\"role\"] == \"user\"])\n",
    "                stats[split] = {\n",
    "                    \"conversations\": len(convs),\n",
    "                    \"total_qa_turns\": total_turns,\n",
    "                    \"density_distribution\": density_dist,\n",
    "                }\n",
    "        return stats\n",
    "\n",
    "print(\"âœ… QA generator ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2a61aa",
   "metadata": {},
   "source": [
    "## 6 Â· YOLO Building Detector Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e0b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOTrainer:\n",
    "    \"\"\"\n",
    "    Disk-aware YOLO trainer.\n",
    "    - Cleans old checkpoints after each save_period\n",
    "    - Monitors disk throughout training\n",
    "    - Conservative batch=8 for T4\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_yaml: Path, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.yolo_cfg = cfg.yolo\n",
    "        self.dataset_yaml = dataset_yaml\n",
    "        self.model = None\n",
    "        self._wandb_run = None\n",
    "\n",
    "    def train(self) -> Path:\n",
    "        from ultralytics import YOLO\n",
    "        with Timer(\"YOLO Training\"):\n",
    "            log_vram(\"before YOLO load\")\n",
    "            if IS_KAGGLE:\n",
    "                log_disk(\"before YOLO train\")\n",
    "\n",
    "            resume_weights = self._find_resume_weights()\n",
    "            if resume_weights and self.yolo_cfg.resume:\n",
    "                log.info(f\"[YOLO] Resuming from: {resume_weights}\")\n",
    "                self.model = YOLO(str(resume_weights))\n",
    "            else:\n",
    "                log.info(f\"[YOLO] Starting fresh with {self.yolo_cfg.model_variant}\")\n",
    "                self.model = YOLO(self.yolo_cfg.model_variant)\n",
    "            log_vram(\"after YOLO load\")\n",
    "\n",
    "            self._wandb_run = init_wandb(\n",
    "                self.cfg, run_name=\"yolo-building-detector\", tags=[\"yolo\", \"training\"]\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                results = self.model.train(\n",
    "                    data=str(self.dataset_yaml),\n",
    "                    epochs=self.yolo_cfg.epochs,\n",
    "                    batch=self.yolo_cfg.batch_size,\n",
    "                    imgsz=self.yolo_cfg.image_size,\n",
    "                    lr0=self.yolo_cfg.lr0, lrf=self.yolo_cfg.lrf,\n",
    "                    patience=self.yolo_cfg.patience,\n",
    "                    save_period=self.yolo_cfg.save_period,\n",
    "                    workers=self.yolo_cfg.workers,\n",
    "                    device=self.yolo_cfg.device,\n",
    "                    project=str(self.yolo_cfg.project),\n",
    "                    name=self.yolo_cfg.name, exist_ok=True,\n",
    "                    pretrained=True, verbose=True,\n",
    "                    hsv_h=0.015, hsv_s=0.4, hsv_v=0.3,\n",
    "                    flipud=0.3, fliplr=0.5, mosaic=0.8, mixup=0.1,\n",
    "                    plots=True, val=True,\n",
    "                )\n",
    "            except KeyboardInterrupt:\n",
    "                log.warning(\"[YOLO] Training interrupted. Weights are saved.\")\n",
    "            finally:\n",
    "                finish_wandb()\n",
    "                # Clean up old checkpoints to save disk\n",
    "                run_dir = self.yolo_cfg.project / self.yolo_cfg.name\n",
    "                if run_dir.exists():\n",
    "                    # Delete YOLO's auto-saved epoch weights except best/last\n",
    "                    for pt in (run_dir / \"weights\").glob(\"epoch*.pt\"):\n",
    "                        pt.unlink(missing_ok=True)\n",
    "                    log.info(\"[YOLO] Cleaned up intermediate epoch weights.\")\n",
    "\n",
    "            best_weights = self._get_best_weights()\n",
    "            log.info(f\"[YOLO] âœ“ Best weights: {best_weights}\")\n",
    "            log_vram(\"after YOLO training\")\n",
    "            if IS_KAGGLE:\n",
    "                log_disk(\"after YOLO train\")\n",
    "            return best_weights\n",
    "\n",
    "    def validate(self) -> Dict:\n",
    "        from ultralytics import YOLO\n",
    "        best = self._get_best_weights()\n",
    "        if not best.exists():\n",
    "            log.error(\"[YOLO] No trained weights found.\")\n",
    "            return {}\n",
    "        model = YOLO(str(best))\n",
    "        results = model.val(\n",
    "            data=str(self.dataset_yaml), batch=self.yolo_cfg.batch_size,\n",
    "            imgsz=self.yolo_cfg.image_size, device=self.yolo_cfg.device,\n",
    "        )\n",
    "        metrics = {\n",
    "            \"mAP50\": results.box.map50 if hasattr(results.box, 'map50') else 0.0,\n",
    "            \"mAP50-95\": results.box.map if hasattr(results.box, 'map') else 0.0,\n",
    "            \"precision\": results.box.mp if hasattr(results.box, 'mp') else 0.0,\n",
    "            \"recall\": results.box.mr if hasattr(results.box, 'mr') else 0.0,\n",
    "        }\n",
    "        metrics[\"f1\"] = 2 * metrics[\"precision\"] * metrics[\"recall\"] / max(metrics[\"precision\"] + metrics[\"recall\"], 1e-6)\n",
    "        log.info(f\"[YOLO] Validation metrics: {metrics}\")\n",
    "        return metrics\n",
    "\n",
    "    def _find_resume_weights(self):\n",
    "        run_dir = self.yolo_cfg.project / self.yolo_cfg.name\n",
    "        last_weights = run_dir / \"weights\" / \"last.pt\"\n",
    "        if last_weights.exists():\n",
    "            return last_weights\n",
    "        return None\n",
    "\n",
    "    def _get_best_weights(self) -> Path:\n",
    "        return self.yolo_cfg.project / self.yolo_cfg.name / \"weights\" / \"best.pt\"\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.model is not None:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "        nuke_vram()\n",
    "        log.info(\"[YOLO] Model unloaded, VRAM nuked.\")\n",
    "\n",
    "print(\"âœ… YOLO trainer ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf72c96",
   "metadata": {},
   "source": [
    "## 7 Â· VLM Trainer (Qwen2-VL + LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset as TorchDataset\n",
    "\n",
    "\n",
    "class GeoExtractVLMDataset(TorchDataset):\n",
    "    def __init__(self, jsonl_path: Path, processor, max_length: int = 512, include_images: bool = True):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.include_images = include_images\n",
    "        self.conversations = []\n",
    "        with open(jsonl_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line:\n",
    "                    self.conversations.append(json.loads(line))\n",
    "        log.info(f\"[VLM Dataset] Loaded {len(self.conversations)} conversations from {jsonl_path.name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.conversations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        conv = self.conversations[idx]\n",
    "        messages = conv[\"messages\"]\n",
    "        image_path = conv.get(\"image\")\n",
    "        formatted_messages = []\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            content = msg[\"content\"]\n",
    "            if role == \"user\" and self.include_images and image_path:\n",
    "                formatted_messages.append({\n",
    "                    \"role\": role,\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\", \"image\": image_path},\n",
    "                        {\"type\": \"text\", \"text\": content},\n",
    "                    ],\n",
    "                })\n",
    "                image_path = None  # Only first user message gets the image\n",
    "            else:\n",
    "                formatted_messages.append({\"role\": role, \"content\": [{\"type\": \"text\", \"text\": content}]})\n",
    "        return {\"messages\": formatted_messages, \"id\": conv.get(\"id\", str(idx))}\n",
    "\n",
    "\n",
    "class ChatMLCollator:\n",
    "    def __init__(self, processor, max_length: int = 512):\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = processor.tokenizer if hasattr(processor, 'tokenizer') else processor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        texts, images = [], []\n",
    "        for sample in batch:\n",
    "            messages = sample[\"messages\"]\n",
    "            try:\n",
    "                text = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "            except Exception:\n",
    "                text = self._manual_chatml(messages)\n",
    "            texts.append(text)\n",
    "            for msg in messages:\n",
    "                if isinstance(msg.get(\"content\"), list):\n",
    "                    for part in msg[\"content\"]:\n",
    "                        if isinstance(part, dict) and part.get(\"type\") == \"image\":\n",
    "                            img_path = part.get(\"image\", \"\")\n",
    "                            if img_path and Path(img_path).exists():\n",
    "                                from PIL import Image\n",
    "                                try:\n",
    "                                    images.append(Image.open(img_path).convert(\"RGB\"))\n",
    "                                except Exception:\n",
    "                                    pass\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            texts, padding=True, truncation=True,\n",
    "            max_length=self.max_length, return_tensors=\"pt\",\n",
    "        )\n",
    "        labels = encoding[\"input_ids\"].clone()\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "        labels = self._mask_non_assistant_tokens(texts, labels)\n",
    "        encoding[\"labels\"] = labels\n",
    "        return encoding\n",
    "\n",
    "    def _mask_non_assistant_tokens(self, texts, labels):\n",
    "        for i, text in enumerate(texts):\n",
    "            assistant_start_token = \"<|im_start|>assistant\"\n",
    "            assistant_end_token = \"<|im_end|>\"\n",
    "            char_pos = 0\n",
    "            assistant_ranges = []\n",
    "            while True:\n",
    "                start_idx = text.find(assistant_start_token, char_pos)\n",
    "                if start_idx == -1:\n",
    "                    break\n",
    "                content_start = text.find(\"\\n\", start_idx)\n",
    "                if content_start == -1:\n",
    "                    break\n",
    "                content_start += 1\n",
    "                end_idx = text.find(assistant_end_token, content_start)\n",
    "                if end_idx == -1:\n",
    "                    end_idx = len(text)\n",
    "                assistant_ranges.append((content_start, end_idx))\n",
    "                char_pos = end_idx + len(assistant_end_token)\n",
    "\n",
    "            if assistant_ranges:\n",
    "                mask = torch.ones_like(labels[i], dtype=torch.bool)\n",
    "                for start, end in assistant_ranges:\n",
    "                    prefix_tokens = self.tokenizer.encode(text[:start], add_special_tokens=False)\n",
    "                    content_tokens = self.tokenizer.encode(text[start:end], add_special_tokens=False)\n",
    "                    tok_start = min(len(prefix_tokens), labels.shape[1] - 1)\n",
    "                    tok_end = min(len(prefix_tokens) + len(content_tokens), labels.shape[1])\n",
    "                    mask[tok_start:tok_end] = False\n",
    "                labels[i][mask] = -100\n",
    "        return labels\n",
    "\n",
    "    def _manual_chatml(self, messages):\n",
    "        parts = []\n",
    "        for msg in messages:\n",
    "            role = msg[\"role\"]\n",
    "            if isinstance(msg[\"content\"], list):\n",
    "                content = \" \".join(\n",
    "                    p[\"text\"] for p in msg[\"content\"]\n",
    "                    if isinstance(p, dict) and p.get(\"type\") == \"text\"\n",
    "                )\n",
    "            else:\n",
    "                content = msg[\"content\"]\n",
    "            parts.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\")\n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "class VLMTrainer:\n",
    "    \"\"\"\n",
    "    T4 OOM-proof VLM Trainer.\n",
    "    CRITICAL changes from previous version:\n",
    "    - FORCES eager attention (T4 SM 7.5 cannot do flash_attention_2)\n",
    "    - batch_size=1, gradient_accumulation=16\n",
    "    - max_seq_length=512 (halved)\n",
    "    - gradient_checkpointing=True (mandatory)\n",
    "    - save_total_limit=2 (disk safety)\n",
    "    - Cleans old checkpoints aggressively\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.vlm_cfg = cfg.vlm\n",
    "        self.model = None\n",
    "        self.processor = None\n",
    "        self.trainer = None\n",
    "\n",
    "    def train(self) -> Path:\n",
    "        with Timer(\"VLM Training\"):\n",
    "            log_vram(\"before VLM load\")\n",
    "            if IS_KAGGLE:\n",
    "                log_disk(\"before VLM train\")\n",
    "            self._load_model()\n",
    "            log_vram(\"after VLM load\")\n",
    "            self._apply_lora()\n",
    "            train_dataset, val_dataset = self._load_datasets()\n",
    "            self._setup_trainer(train_dataset, val_dataset)\n",
    "            self._run_training()\n",
    "            final_path = self._save_final()\n",
    "            log.info(f\"[VLM] âœ“ Training complete. Adapter saved to {final_path}\")\n",
    "            if IS_KAGGLE:\n",
    "                log_disk(\"after VLM train\")\n",
    "            return final_path\n",
    "\n",
    "    def _load_model(self):\n",
    "        from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=self.vlm_cfg.load_in_4bit,\n",
    "            bnb_4bit_quant_type=self.vlm_cfg.bnb_4bit_quant_type,\n",
    "            bnb_4bit_compute_dtype=getattr(torch, self.vlm_cfg.bnb_4bit_compute_dtype),\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        # â”€â”€ FORCE eager attention on T4 (SM 7.5 < 8.0) â”€â”€\n",
    "        device_cap = torch.cuda.get_device_capability()[0] if torch.cuda.is_available() else 0\n",
    "        attn_impl = \"flash_attention_2\" if device_cap >= 8 else \"eager\"\n",
    "        log.info(f\"[VLM] Attention implementation: {attn_impl} (SM {device_cap}.x)\")\n",
    "\n",
    "        log.info(f\"[VLM] Loading {self.vlm_cfg.model_id} in 4-bit NF4...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.vlm_cfg.model_id,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            attn_implementation=attn_impl,\n",
    "        )\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            self.vlm_cfg.model_id, trust_remote_code=True,\n",
    "        )\n",
    "        if self.processor.tokenizer.pad_token is None:\n",
    "            self.processor.tokenizer.pad_token = self.processor.tokenizer.eos_token\n",
    "            self.model.config.pad_token_id = self.model.config.eos_token_id\n",
    "        params = count_parameters(self.model)\n",
    "        log.info(f\"[VLM] Model loaded. Total: {params['total']:,}, Trainable (pre-LoRA): {params['trainable']:,}\")\n",
    "\n",
    "    def _apply_lora(self):\n",
    "        from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "\n",
    "        # gradient_checkpointing MUST be True for T4\n",
    "        self.model = prepare_model_for_kbit_training(\n",
    "            self.model, use_gradient_checkpointing=True,\n",
    "        )\n",
    "        lora_config = LoraConfig(\n",
    "            r=self.vlm_cfg.lora_r,\n",
    "            lora_alpha=self.vlm_cfg.lora_alpha,\n",
    "            lora_dropout=self.vlm_cfg.lora_dropout,\n",
    "            target_modules=self.vlm_cfg.lora_target_modules,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        params = count_parameters(self.model)\n",
    "        log.info(f\"[VLM] LoRA applied. Trainable: {params['trainable']:,} ({params['trainable_pct']}%)\")\n",
    "        self.model.print_trainable_parameters()\n",
    "\n",
    "    def _load_datasets(self):\n",
    "        train_path = VLM_DATA_DIR / \"train.jsonl\"\n",
    "        val_path = VLM_DATA_DIR / \"val.jsonl\"\n",
    "        if not train_path.exists():\n",
    "            raise FileNotFoundError(f\"Training data not found at {train_path}. Run QA generator first!\")\n",
    "        train_dataset = GeoExtractVLMDataset(\n",
    "            train_path, self.processor,\n",
    "            max_length=self.vlm_cfg.max_seq_length,  # 512\n",
    "        )\n",
    "        val_dataset = None\n",
    "        if val_path.exists():\n",
    "            val_dataset = GeoExtractVLMDataset(\n",
    "                val_path, self.processor,\n",
    "                max_length=self.vlm_cfg.max_seq_length,\n",
    "            )\n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def _setup_trainer(self, train_dataset, val_dataset):\n",
    "        from transformers import TrainingArguments, Trainer\n",
    "\n",
    "        resume_ckpt = None\n",
    "        if self.vlm_cfg.resume_from_checkpoint:\n",
    "            resume_ckpt = find_latest_checkpoint(Path(self.vlm_cfg.output_dir))\n",
    "            if resume_ckpt:\n",
    "                log.info(f\"[VLM] Will resume from: {resume_ckpt}\")\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.vlm_cfg.output_dir),\n",
    "            num_train_epochs=self.vlm_cfg.epochs,\n",
    "            per_device_train_batch_size=1,       # â† MUST be 1 for T4\n",
    "            per_device_eval_batch_size=1,         # â† MUST be 1 for T4\n",
    "            gradient_accumulation_steps=self.vlm_cfg.gradient_accumulation_steps,  # 16\n",
    "            learning_rate=self.vlm_cfg.learning_rate,\n",
    "            weight_decay=self.vlm_cfg.weight_decay,\n",
    "            warmup_ratio=self.vlm_cfg.warmup_ratio,\n",
    "            lr_scheduler_type=self.vlm_cfg.lr_scheduler_type,\n",
    "            fp16=True,                            # T4 must use fp16\n",
    "            bf16=False,                           # T4 cannot do bf16\n",
    "            save_steps=self.vlm_cfg.save_steps,\n",
    "            save_total_limit=2,                   # â† CRITICAL: only keep 2 checkpoints\n",
    "            save_strategy=\"steps\",\n",
    "            eval_strategy=\"steps\" if val_dataset else \"no\",\n",
    "            eval_steps=self.vlm_cfg.eval_steps if val_dataset else None,\n",
    "            logging_steps=self.vlm_cfg.logging_steps,\n",
    "            logging_first_step=True,\n",
    "            report_to=\"wandb\" if self.cfg.wandb.enabled else \"none\",\n",
    "            run_name=\"vlm-geoextract\",\n",
    "            gradient_checkpointing=True,          # â† MANDATORY for T4\n",
    "            optim=\"paged_adamw_8bit\",\n",
    "            max_grad_norm=1.0,\n",
    "            remove_unused_columns=False,\n",
    "            dataloader_num_workers=0,             # â† 0 workers = less RAM overhead\n",
    "            dataloader_pin_memory=False,          # â† Save memory\n",
    "            seed=self.cfg.data.seed,\n",
    "            load_best_model_at_end=True if val_dataset else False,\n",
    "            metric_for_best_model=\"eval_loss\" if val_dataset else None,\n",
    "        )\n",
    "\n",
    "        collator = ChatMLCollator(self.processor, max_length=self.vlm_cfg.max_seq_length)\n",
    "\n",
    "        self.trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            data_collator=collator,\n",
    "        )\n",
    "        self._resume_checkpoint = resume_ckpt\n",
    "\n",
    "    def _run_training(self):\n",
    "        wandb_run = init_wandb(\n",
    "            self.cfg, run_name=\"vlm-geoextract-lora\",\n",
    "            tags=[\"vlm\", \"lora\", \"qwen2-vl\", \"t4-optimized\"],\n",
    "        )\n",
    "        try:\n",
    "            if self._resume_checkpoint:\n",
    "                log.info(f\"[VLM] Resuming from {self._resume_checkpoint}\")\n",
    "                self.trainer.train(resume_from_checkpoint=str(self._resume_checkpoint))\n",
    "            else:\n",
    "                log.info(\"[VLM] Starting training from scratch.\")\n",
    "                self.trainer.train()\n",
    "        except KeyboardInterrupt:\n",
    "            log.warning(\"[VLM] Training interrupted. Saving checkpoint...\")\n",
    "            self.trainer.save_model(str(self.vlm_cfg.output_dir / \"interrupted\"))\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            log.error(\"[VLM] â›” OOM! Saving what we can...\")\n",
    "            nuke_vram()\n",
    "            self.trainer.save_model(str(self.vlm_cfg.output_dir / \"oom_rescue\"))\n",
    "        finally:\n",
    "            finish_wandb()\n",
    "            # Clean up old checkpoints to save disk\n",
    "            cleanup_old_checkpoints(Path(self.vlm_cfg.output_dir), keep=2)\n",
    "\n",
    "    def _save_final(self) -> Path:\n",
    "        final_dir = Path(self.vlm_cfg.output_dir) / \"final_adapter\"\n",
    "        final_dir.mkdir(parents=True, exist_ok=True)\n",
    "        self.model.save_pretrained(str(final_dir))\n",
    "        self.processor.save_pretrained(str(final_dir))\n",
    "        log.info(f\"[VLM] Final adapter saved to {final_dir}\")\n",
    "\n",
    "        # Clean up intermediate checkpoints after saving final\n",
    "        cleanup_old_checkpoints(Path(self.vlm_cfg.output_dir), keep=1)\n",
    "        if IS_KAGGLE:\n",
    "            log_disk(\"after VLM save\")\n",
    "        return final_dir\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.model is not None:\n",
    "            del self.model; self.model = None\n",
    "        if self.processor is not None:\n",
    "            del self.processor; self.processor = None\n",
    "        if self.trainer is not None:\n",
    "            del self.trainer; self.trainer = None\n",
    "        nuke_vram()\n",
    "        log.info(\"[VLM] Model unloaded, VRAM nuked.\")\n",
    "\n",
    "print(\"âœ… VLM trainer ready (T4 OOM-proof).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f72ef",
   "metadata": {},
   "source": [
    "## 8 Â· Agentic Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21dda87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PILImage\n",
    "\n",
    "\n",
    "class GeoExtractPipeline:\n",
    "    \"\"\"\n",
    "    Memory-safe agentic inference pipeline.\n",
    "    Loads YOLO â†’ runs â†’ unloads â†’ loads VLM â†’ runs â†’ unloads.\n",
    "    Never holds both models in VRAM simultaneously on T4.\n",
    "    \"\"\"\n",
    "    def __init__(self, cfg: InferenceConfig = CFG.inference, full_cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.full_cfg = full_cfg\n",
    "        self.yolo_model = None\n",
    "        self.vlm_model = None\n",
    "        self.vlm_processor = None\n",
    "        self._loaded = False\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Load both models. On T4, this is tight â€” ~14 GB total.\"\"\"\n",
    "        log_vram(\"before pipeline load\")\n",
    "        self._load_yolo()\n",
    "        self._load_vlm()\n",
    "        self._loaded = True\n",
    "        log_vram(\"after pipeline load\")\n",
    "        log.info(\"[Pipeline] âœ“ Both models loaded and ready.\")\n",
    "\n",
    "    def _load_yolo(self):\n",
    "        from ultralytics import YOLO\n",
    "        weights_path = self.cfg.yolo_weights\n",
    "        if not weights_path.exists():\n",
    "            candidates = list(self.cfg.yolo_weights.parent.parent.rglob(\"best.pt\"))\n",
    "            if candidates:\n",
    "                weights_path = candidates[0]\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"YOLO weights not found at {self.cfg.yolo_weights}\")\n",
    "        self.yolo_model = YOLO(str(weights_path))\n",
    "        log.info(f\"[Pipeline] YOLO loaded from {weights_path}\")\n",
    "\n",
    "    def _load_vlm(self):\n",
    "        from transformers import AutoModelForCausalLM, AutoProcessor, BitsAndBytesConfig\n",
    "        from peft import PeftModel\n",
    "\n",
    "        vlm_cfg = self.full_cfg.vlm\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "\n",
    "        device_cap = torch.cuda.get_device_capability()[0] if torch.cuda.is_available() else 0\n",
    "        attn_impl = \"flash_attention_2\" if device_cap >= 8 else \"eager\"\n",
    "\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            vlm_cfg.model_id, quantization_config=bnb_config,\n",
    "            device_map=\"auto\", trust_remote_code=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            attn_implementation=attn_impl,\n",
    "        )\n",
    "\n",
    "        adapter_dir = self.cfg.vlm_adapter_dir / \"final_adapter\"\n",
    "        if not adapter_dir.exists():\n",
    "            candidates = list(self.cfg.vlm_adapter_dir.rglob(\"adapter_config.json\"))\n",
    "            if candidates:\n",
    "                adapter_dir = candidates[0].parent\n",
    "            else:\n",
    "                log.warning(\"[Pipeline] No LoRA adapter found â€” using base model.\")\n",
    "                self.vlm_model = base_model\n",
    "                self.vlm_processor = AutoProcessor.from_pretrained(vlm_cfg.model_id, trust_remote_code=True)\n",
    "                return\n",
    "\n",
    "        self.vlm_model = PeftModel.from_pretrained(base_model, str(adapter_dir))\n",
    "        self.vlm_model.eval()\n",
    "        self.vlm_processor = AutoProcessor.from_pretrained(str(adapter_dir), trust_remote_code=True)\n",
    "        if self.vlm_processor.tokenizer.pad_token is None:\n",
    "            self.vlm_processor.tokenizer.pad_token = self.vlm_processor.tokenizer.eos_token\n",
    "        log.info(f\"[Pipeline] VLM loaded with adapter from {adapter_dir}\")\n",
    "\n",
    "    def analyze(self, image, question=None):\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        start_time = time.time()\n",
    "        if isinstance(image, (str, Path)):\n",
    "            image_path = str(image)\n",
    "            pil_image = PILImage.open(image_path).convert(\"RGB\")\n",
    "        else:\n",
    "            pil_image = image\n",
    "            image_path = \"uploaded_image\"\n",
    "\n",
    "        yolo_results = self._run_yolo(pil_image)\n",
    "        context = self._build_context(yolo_results)\n",
    "        if question is None:\n",
    "            question = (\n",
    "                \"Analyze this satellite image comprehensively. Assess the building \"\n",
    "                \"density, urban heat island risk, green space availability, \"\n",
    "                \"infrastructure stress, and provide urban planning recommendations.\"\n",
    "            )\n",
    "        vlm_response = self._run_vlm(pil_image, question, context)\n",
    "        return {\n",
    "            \"image\": image_path,\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "            \"processing_time_s\": round(time.time() - start_time, 2),\n",
    "            \"detection\": yolo_results,\n",
    "            \"context\": context,\n",
    "            \"analysis\": {\"question\": question, \"response\": vlm_response},\n",
    "            \"metadata\": {\n",
    "                \"yolo_model\": str(self.cfg.yolo_weights.name),\n",
    "                \"vlm_model\": self.full_cfg.vlm.model_id,\n",
    "                \"confidence_threshold\": self.cfg.confidence_threshold,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def batch_analyze(self, images, question=None):\n",
    "        results = []\n",
    "        for img in images:\n",
    "            try:\n",
    "                results.append(self.analyze(img, question))\n",
    "            except Exception as e:\n",
    "                results.append({\"image\": str(img), \"error\": str(e)})\n",
    "            # Free intermediate VRAM between images\n",
    "            free_vram()\n",
    "        return results\n",
    "\n",
    "    def _run_yolo(self, image):\n",
    "        results = self.yolo_model(\n",
    "            image, conf=self.cfg.confidence_threshold,\n",
    "            iou=self.cfg.iou_threshold, verbose=False,\n",
    "        )\n",
    "        detections = []\n",
    "        if results and len(results) > 0:\n",
    "            boxes = results[0].boxes\n",
    "            if boxes is not None:\n",
    "                for i in range(len(boxes)):\n",
    "                    detections.append({\n",
    "                        \"bbox\": boxes.xyxy[i].cpu().tolist(),\n",
    "                        \"confidence\": float(boxes.conf[i].cpu()),\n",
    "                        \"class\": int(boxes.cls[i].cpu()),\n",
    "                        \"class_name\": \"building\",\n",
    "                    })\n",
    "        return {\n",
    "            \"building_count\": len(detections),\n",
    "            \"detections\": detections,\n",
    "            \"avg_confidence\": round(np.mean([d[\"confidence\"] for d in detections]), 3) if detections else 0.0,\n",
    "        }\n",
    "\n",
    "    def _build_context(self, yolo_results):\n",
    "        count = yolo_results[\"building_count\"]\n",
    "        qa_cfg = self.full_cfg.qa\n",
    "        if count <= qa_cfg.sparse_max:\n",
    "            density = \"Sparse\"\n",
    "        elif count <= qa_cfg.moderate_max:\n",
    "            density = \"Moderate\"\n",
    "        elif count <= qa_cfg.dense_max:\n",
    "            density = \"Dense\"\n",
    "        else:\n",
    "            density = \"Urban Core\"\n",
    "        return {\n",
    "            \"building_count\": count, \"density_class\": density,\n",
    "            \"avg_detection_confidence\": yolo_results[\"avg_confidence\"],\n",
    "            \"context_prompt\": (\n",
    "                f\"The building detection model has identified {count} structures \"\n",
    "                f\"in this image with an average confidence of \"\n",
    "                f\"{yolo_results['avg_confidence']:.1%}. This area is classified \"\n",
    "                f\"as '{density}' density.\"\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    def _run_vlm(self, image, question, context):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.full_cfg.qa.system_prompt},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "                {\"type\": \"text\", \"text\": f\"Context from detection model: {context['context_prompt']}\\n\\nQuestion: {question}\"},\n",
    "            ]},\n",
    "        ]\n",
    "        try:\n",
    "            text = self.vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        except Exception:\n",
    "            text = (\n",
    "                f\"<|im_start|>system\\n{self.full_cfg.qa.system_prompt}<|im_end|>\\n\"\n",
    "                f\"<|im_start|>user\\n{context['context_prompt']}\\n{question}<|im_end|>\\n\"\n",
    "                f\"<|im_start|>assistant\\n\"\n",
    "            )\n",
    "        inputs = self.vlm_processor(\n",
    "            text=[text], images=[image], return_tensors=\"pt\", padding=True,\n",
    "        ).to(self.vlm_model.device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            outputs = self.vlm_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=self.cfg.max_new_tokens,\n",
    "                do_sample=True, temperature=0.7, top_p=0.9,\n",
    "                repetition_penalty=1.1,\n",
    "            )\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        response = self.vlm_processor.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Explicitly free inference tensors\n",
    "        del inputs, outputs\n",
    "        free_vram()\n",
    "        return response\n",
    "\n",
    "    def chat(self, image, conversation_history, new_question):\n",
    "        if not self._loaded:\n",
    "            self.load()\n",
    "        if isinstance(image, (str, Path)):\n",
    "            pil_image = PILImage.open(str(image)).convert(\"RGB\")\n",
    "        else:\n",
    "            pil_image = image\n",
    "        if not conversation_history:\n",
    "            yolo_results = self._run_yolo(pil_image)\n",
    "            context = self._build_context(yolo_results)\n",
    "        else:\n",
    "            context = conversation_history[0].get(\"context\", {})\n",
    "\n",
    "        messages = [{\"role\": \"system\", \"content\": self.full_cfg.qa.system_prompt}]\n",
    "        for turn in conversation_history:\n",
    "            messages.append({\"role\": \"user\", \"content\": turn.get(\"question\", \"\")})\n",
    "            messages.append({\"role\": \"assistant\", \"content\": turn.get(\"response\", \"\")})\n",
    "        messages.append({\"role\": \"user\", \"content\": new_question})\n",
    "\n",
    "        text = self.vlm_processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.vlm_processor(\n",
    "            text=[text], images=[pil_image], return_tensors=\"pt\", padding=True,\n",
    "        ).to(self.vlm_model.device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.float16):\n",
    "            outputs = self.vlm_model.generate(\n",
    "                **inputs, max_new_tokens=self.cfg.max_new_tokens,\n",
    "                do_sample=True, temperature=0.7,\n",
    "            )\n",
    "        input_len = inputs[\"input_ids\"].shape[1]\n",
    "        response = self.vlm_processor.tokenizer.decode(outputs[0][input_len:], skip_special_tokens=True).strip()\n",
    "        conversation_history.append({\"question\": new_question, \"response\": response, \"context\": context})\n",
    "\n",
    "        del inputs, outputs\n",
    "        free_vram()\n",
    "        return {\"response\": response, \"conversation_history\": conversation_history}\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self.yolo_model is not None:\n",
    "            del self.yolo_model; self.yolo_model = None\n",
    "        if self.vlm_model is not None:\n",
    "            del self.vlm_model; self.vlm_model = None\n",
    "        if self.vlm_processor is not None:\n",
    "            del self.vlm_processor; self.vlm_processor = None\n",
    "        self._loaded = False\n",
    "        nuke_vram()\n",
    "        log.info(\"[Pipeline] All models unloaded.\")\n",
    "\n",
    "print(\"âœ… Inference pipeline ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aed9705",
   "metadata": {},
   "source": [
    "## 9 Â· Evaluation Module (Defense Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd7ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, accuracy_score,\n",
    ")\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class YOLOEvaluator:\n",
    "    def __init__(self, iou_threshold: float = 0.5):\n",
    "        self.iou_threshold = iou_threshold\n",
    "\n",
    "    def evaluate(self, dataset_yaml: Path, weights_path: Path) -> Dict:\n",
    "        from ultralytics import YOLO\n",
    "        log.info(f\"[Eval-YOLO] Running validation with IoU={self.iou_threshold}...\")\n",
    "        model = YOLO(str(weights_path))\n",
    "        results = model.val(\n",
    "            data=str(dataset_yaml), iou=self.iou_threshold,\n",
    "            conf=0.25, verbose=True, plots=True,\n",
    "            save_dir=str(EVAL_DIR / \"yolo_eval\"),\n",
    "        )\n",
    "        metrics = {\n",
    "            \"precision\": float(results.box.mp) if hasattr(results.box, 'mp') else 0.0,\n",
    "            \"recall\": float(results.box.mr) if hasattr(results.box, 'mr') else 0.0,\n",
    "            \"mAP50\": float(results.box.map50) if hasattr(results.box, 'map50') else 0.0,\n",
    "            \"mAP50-95\": float(results.box.map) if hasattr(results.box, 'map') else 0.0,\n",
    "        }\n",
    "        p, r = metrics[\"precision\"], metrics[\"recall\"]\n",
    "        metrics[\"f1\"] = 2 * p * r / max(p + r, 1e-8)\n",
    "        log.info(f\"[Eval-YOLO] Results: {metrics}\")\n",
    "        out_path = EVAL_DIR / \"yolo_metrics.json\"\n",
    "        out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(out_path, \"w\") as f:\n",
    "            json.dump(metrics, f, indent=2)\n",
    "        del model\n",
    "        free_vram()\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_counting_accuracy(self, predictions, ground_truths) -> Dict:\n",
    "        predictions, ground_truths = np.array(predictions), np.array(ground_truths)\n",
    "        mae = float(np.mean(np.abs(predictions - ground_truths)))\n",
    "        rmse = float(np.sqrt(np.mean((predictions - ground_truths) ** 2)))\n",
    "        mape = float(np.mean(np.abs(predictions - ground_truths) / np.maximum(ground_truths, 1)) * 100)\n",
    "        metrics = {\n",
    "            \"mae\": mae, \"rmse\": rmse, \"mape_pct\": mape,\n",
    "            \"exact_match\": float(np.mean(predictions == ground_truths)),\n",
    "            \"within_5\": float(np.mean(np.abs(predictions - ground_truths) <= 5)),\n",
    "            \"within_10\": float(np.mean(np.abs(predictions - ground_truths) <= 10)),\n",
    "        }\n",
    "        log.info(f\"[Eval-YOLO] Counting accuracy: {metrics}\")\n",
    "        return metrics\n",
    "\n",
    "\n",
    "class VLMEvaluator:\n",
    "    DENSITY_CLASSES = [\"Sparse\", \"Moderate\", \"Dense\", \"Urban Core\"]\n",
    "\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(self.DENSITY_CLASSES)}\n",
    "\n",
    "    def evaluate(self, gt_classes, pred_classes) -> Dict:\n",
    "        log.info(f\"[Eval-VLM] Evaluating {len(gt_classes)} samples...\")\n",
    "        gt_idx = [self.class_to_idx.get(c, -1) for c in gt_classes]\n",
    "        pred_idx = [self.class_to_idx.get(c, -1) for c in pred_classes]\n",
    "        valid = [(g, p) for g, p in zip(gt_idx, pred_idx) if g >= 0 and p >= 0]\n",
    "        if not valid:\n",
    "            log.error(\"[Eval-VLM] No valid predictions!\")\n",
    "            return {}\n",
    "        gt_valid = [v[0] for v in valid]\n",
    "        pred_valid = [v[1] for v in valid]\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": float(accuracy_score(gt_valid, pred_valid)),\n",
    "            \"f1_macro\": float(f1_score(gt_valid, pred_valid, average=\"macro\", zero_division=0)),\n",
    "            \"f1_weighted\": float(f1_score(gt_valid, pred_valid, average=\"weighted\", zero_division=0)),\n",
    "            \"precision_macro\": float(precision_score(gt_valid, pred_valid, average=\"macro\", zero_division=0)),\n",
    "            \"recall_macro\": float(recall_score(gt_valid, pred_valid, average=\"macro\", zero_division=0)),\n",
    "        }\n",
    "        report = classification_report(\n",
    "            gt_valid, pred_valid, target_names=self.DENSITY_CLASSES,\n",
    "            output_dict=True, zero_division=0,\n",
    "        )\n",
    "        metrics[\"per_class\"] = {\n",
    "            cls: {\"precision\": report[cls][\"precision\"], \"recall\": report[cls][\"recall\"],\n",
    "                  \"f1\": report[cls][\"f1-score\"], \"support\": report[cls][\"support\"]}\n",
    "            for cls in self.DENSITY_CLASSES if cls in report\n",
    "        }\n",
    "        cm = confusion_matrix(gt_valid, pred_valid, labels=list(range(len(self.DENSITY_CLASSES))))\n",
    "        metrics[\"confusion_matrix\"] = cm.tolist()\n",
    "\n",
    "        log.info(f\"[Eval-VLM] Accuracy: {metrics['accuracy']:.4f}, F1-macro: {metrics['f1_macro']:.4f}\")\n",
    "        print(\"\\n\" + classification_report(gt_valid, pred_valid, target_names=self.DENSITY_CLASSES, zero_division=0))\n",
    "\n",
    "        # Save confusion matrix plot\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        im = ax.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        ax.set(xticks=np.arange(cm.shape[1]), yticks=np.arange(cm.shape[0]),\n",
    "               xticklabels=self.DENSITY_CLASSES, yticklabels=self.DENSITY_CLASSES,\n",
    "               title=\"Density Classification â€” Confusion Matrix\",\n",
    "               ylabel=\"Ground Truth\", xlabel=\"Predicted\")\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\")\n",
    "        thresh = cm.max() / 2.0\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], \"d\"), ha=\"center\", va=\"center\",\n",
    "                        color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(str(EVAL_DIR / \"vlm_confusion_matrix.png\"), dpi=150)\n",
    "        plt.close(fig)\n",
    "\n",
    "        # Save metrics\n",
    "        serializable = {k: v for k, v in metrics.items() if not isinstance(v, np.ndarray)}\n",
    "        with open(EVAL_DIR / \"vlm_metrics.json\", \"w\") as f:\n",
    "            json.dump(serializable, f, indent=2, default=str)\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_from_pipeline(self, pipeline, val_jsonl=None, max_samples=None):\n",
    "        if max_samples is None:\n",
    "            max_samples = CFG.evaluation.max_eval_samples\n",
    "        if val_jsonl is None:\n",
    "            val_jsonl = VLM_DATA_DIR / \"val.jsonl\"\n",
    "        if not val_jsonl.exists():\n",
    "            log.error(f\"[Eval-VLM] Validation data not found: {val_jsonl}\")\n",
    "            return {}\n",
    "        with open(val_jsonl) as f:\n",
    "            val_data = [json.loads(l) for l in f.readlines()[:max_samples]]\n",
    "        gt_classes, pred_classes = [], []\n",
    "        for sample in tqdm(val_data, desc=\"VLM Evaluation\"):\n",
    "            gt_class = sample.get(\"density_class\", \"Unknown\")\n",
    "            image_path = sample.get(\"image\", \"\")\n",
    "            if not Path(image_path).exists():\n",
    "                continue\n",
    "            try:\n",
    "                result = pipeline.analyze(image_path)\n",
    "                pred_class = result.get(\"context\", {}).get(\"density_class\", \"Unknown\")\n",
    "                gt_classes.append(gt_class)\n",
    "                pred_classes.append(pred_class)\n",
    "            except Exception as e:\n",
    "                log.warning(f\"[Eval-VLM] Failed on {image_path}: {e}\")\n",
    "        return self.evaluate(gt_classes, pred_classes)\n",
    "\n",
    "\n",
    "class GeoExtractEvaluator:\n",
    "    def __init__(self, cfg=CFG):\n",
    "        self.cfg = cfg\n",
    "        self.yolo_eval = YOLOEvaluator(cfg.evaluation.iou_threshold)\n",
    "        self.vlm_eval = VLMEvaluator(cfg)\n",
    "\n",
    "    def run_full_evaluation(self, dataset_yaml, yolo_weights, pipeline=None):\n",
    "        wandb_run = init_wandb(self.cfg, run_name=\"evaluation\", tags=[\"eval\", \"metrics\"])\n",
    "        all_metrics = {}\n",
    "\n",
    "        with Timer(\"YOLO Evaluation\"):\n",
    "            yolo_metrics = self.yolo_eval.evaluate(dataset_yaml, yolo_weights)\n",
    "            all_metrics[\"yolo\"] = yolo_metrics\n",
    "            if wandb_run:\n",
    "                import wandb\n",
    "                wandb.log({f\"eval/yolo_{k}\": v for k, v in yolo_metrics.items()})\n",
    "\n",
    "        if pipeline is not None:\n",
    "            with Timer(\"VLM Evaluation\"):\n",
    "                vlm_metrics = self.vlm_eval.evaluate_from_pipeline(pipeline)\n",
    "                all_metrics[\"vlm\"] = vlm_metrics\n",
    "                if wandb_run:\n",
    "                    import wandb\n",
    "                    log_m = {k: v for k, v in vlm_metrics.items() if isinstance(v, (int, float))}\n",
    "                    wandb.log({f\"eval/vlm_{k}\": v for k, v in log_m.items()})\n",
    "                    cm_path = EVAL_DIR / \"vlm_confusion_matrix.png\"\n",
    "                    if cm_path.exists():\n",
    "                        wandb.log({\"eval/confusion_matrix\": wandb.Image(str(cm_path))})\n",
    "\n",
    "        finish_wandb()\n",
    "        report_path = EVAL_DIR / \"full_evaluation_report.json\"\n",
    "        with open(report_path, \"w\") as f:\n",
    "            json.dump(all_metrics, f, indent=2, default=str)\n",
    "        log.info(f\"[Eval] âœ“ Full report saved to {report_path}\")\n",
    "        self._print_summary(all_metrics)\n",
    "        return all_metrics\n",
    "\n",
    "    def _print_summary(self, metrics):\n",
    "        print(\"\\n\" + \"â•\" * 60)\n",
    "        print(\"  GeoExtract v2 â€” EVALUATION SUMMARY\")\n",
    "        print(\"â•\" * 60)\n",
    "        if \"yolo\" in metrics:\n",
    "            y = metrics[\"yolo\"]\n",
    "            print(f\"\\n  â”Œâ”€â”€ YOLO Building Detection â”€â”€â”\")\n",
    "            print(f\"  â”‚ Precision:  {y.get('precision', 0):.4f}          â”‚\")\n",
    "            print(f\"  â”‚ Recall:     {y.get('recall', 0):.4f}          â”‚\")\n",
    "            print(f\"  â”‚ F1-Score:   {y.get('f1', 0):.4f}          â”‚\")\n",
    "            print(f\"  â”‚ mAP@50:     {y.get('mAP50', 0):.4f}          â”‚\")\n",
    "            print(f\"  â”‚ mAP@50-95:  {y.get('mAP50-95', 0):.4f}          â”‚\")\n",
    "            print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "        if \"vlm\" in metrics:\n",
    "            v = metrics[\"vlm\"]\n",
    "            print(f\"\\n  â”Œâ”€â”€ VLM Density Classification â”€â”€â”\")\n",
    "            print(f\"  â”‚ Accuracy:   {v.get('accuracy', 0):.4f}             â”‚\")\n",
    "            print(f\"  â”‚ F1 (macro): {v.get('f1_macro', 0):.4f}             â”‚\")\n",
    "            print(f\"  â”‚ F1 (wgt.):  {v.get('f1_weighted', 0):.4f}             â”‚\")\n",
    "            print(f\"  â”‚ Precision:  {v.get('precision_macro', 0):.4f}             â”‚\")\n",
    "            print(f\"  â”‚ Recall:     {v.get('recall_macro', 0):.4f}             â”‚\")\n",
    "            print(f\"  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\")\n",
    "        print(\"â•\" * 60)\n",
    "\n",
    "print(\"âœ… Evaluation module ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3184e37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸš€ PIPELINE EXECUTION\n",
    "\n",
    "All code is loaded. Now run each step sequentially."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30797632",
   "metadata": {},
   "source": [
    "## Step 1 Â· Build YOLO Dataset from SpaceNet 7\n",
    "\n",
    "Parses GeoTIFF images + GeoJSON labels â†’ YOLO bbox format with augmentations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c05f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 1: Build YOLO dataset â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "log_disk(\"Before YOLO dataset build\")\n",
    "log_vram(\"Before YOLO dataset build\")\n",
    "\n",
    "builder = YOLODatasetBuilder(CFG)\n",
    "dataset_yaml = builder.build()\n",
    "\n",
    "stats = builder.get_stats()\n",
    "print(\"\\nğŸ“Š Dataset Statistics:\")\n",
    "for split, s in stats.items():\n",
    "    print(f\"  {split}: {s['images']} images, {s['total_bboxes']} bboxes \"\n",
    "          f\"(avg {s['avg_bboxes_per_image']} per image)\")\n",
    "print(f\"\\nğŸ“ Dataset YAML: {dataset_yaml}\")\n",
    "log_disk(\"After YOLO dataset build\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7793b15d",
   "metadata": {},
   "source": [
    "## Step 2 Â· Generate Synthetic QA Pairs for VLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b4ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 2: Generate QA pairs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "log_disk(\"Before QA generation\")\n",
    "\n",
    "qa_gen = SyntheticQAGenerator(CFG)\n",
    "qa_path = qa_gen.generate()\n",
    "\n",
    "qa_stats = qa_gen.get_stats()\n",
    "print(\"\\nğŸ“Š QA Statistics:\")\n",
    "for split, s in qa_stats.items():\n",
    "    print(f\"  {split}: {s['conversations']} conversations, {s['total_qa_turns']} QA turns\")\n",
    "    print(f\"    Density distribution: {s['density_distribution']}\")\n",
    "log_disk(\"After QA generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cb1bc1",
   "metadata": {},
   "source": [
    "## Step 3 Â· Train YOLO Building Detector\n",
    "\n",
    "**Estimated time: ~45 min â€“ 1.5 hours on T4 GPU (30 epochs, batch=8)**  \n",
    "Model auto-saves every 5 epochs. Safe to interrupt â€” will auto-resume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a631748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 3: Train YOLO â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "log_disk(\"Before YOLO training\")\n",
    "log_vram(\"Before YOLO training\")\n",
    "\n",
    "yolo_trainer = YOLOTrainer(dataset_yaml, CFG)\n",
    "yolo_best_weights = yolo_trainer.train()\n",
    "print(f\"\\nâœ… YOLO training complete. Best weights: {yolo_best_weights}\")\n",
    "log_disk(\"After YOLO training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c70ca1b",
   "metadata": {},
   "source": [
    "## Step 4 Â· Free YOLO VRAM â†’ Train VLM with LoRA\n",
    "\n",
    "**Estimated time: ~2â€“3 hours on T4 GPU (3 epochs, batch=1, grad_accum=16)**  \n",
    "Checkpoints every 200 steps. save_total_limit=2 to respect 20 GB disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998095ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 4a: Free YOLO VRAM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "yolo_trainer.cleanup()\n",
    "del yolo_trainer\n",
    "nuke_vram()\n",
    "print(\"ğŸ§¹ YOLO unloaded. VRAM nuked for VLM.\")\n",
    "log_vram(\"After YOLO cleanup\")\n",
    "log_disk(\"Before VLM training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2137ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 4b: Train VLM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vlm_trainer = VLMTrainer(CFG)\n",
    "vlm_adapter_path = vlm_trainer.train()\n",
    "print(f\"\\nâœ… VLM training complete. Adapter: {vlm_adapter_path}\")\n",
    "log_disk(\"After VLM training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Free VLM trainer for inference â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vlm_trainer.cleanup()\n",
    "del vlm_trainer\n",
    "nuke_vram()\n",
    "print(\"ğŸ§¹ VLM trainer unloaded.\")\n",
    "log_vram(\"Before inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf6b00",
   "metadata": {},
   "source": [
    "## Step 5 Â· Run Agentic Inference Demo\n",
    "\n",
    "YOLO detects buildings â†’ context injected â†’ VLM reasons about the scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1388f64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 5: Agentic inference demo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipeline = GeoExtractPipeline(CFG.inference, CFG)\n",
    "pipeline.load()\n",
    "\n",
    "# Find sample image â€” try JPEG first (our format), then PNG\n",
    "sample_images = (\n",
    "    list(Path(YOLO_DATA_DIR / \"images\" / \"val\").glob(\"*.jpg\")) +\n",
    "    list(Path(YOLO_DATA_DIR / \"images\" / \"val\").glob(\"*.jpeg\")) +\n",
    "    list(Path(YOLO_DATA_DIR / \"images\" / \"val\").glob(\"*.png\"))\n",
    ")\n",
    "if sample_images:\n",
    "    demo_image = sample_images[0]\n",
    "    print(f\"\\nğŸ–¼ï¸ Analyzing: {demo_image.name}\")\n",
    "    result = pipeline.analyze(demo_image)\n",
    "\n",
    "    print(f\"\\nğŸ” Detection: {result['detection']['building_count']} buildings found\")\n",
    "    print(f\"ğŸ“Š Density: {result['context']['density_class']}\")\n",
    "    print(f\"â±ï¸ Processing time: {result['processing_time_s']}s\")\n",
    "    print(f\"\\nğŸ’¬ VLM Analysis:\\n{result['analysis']['response']}\")\n",
    "\n",
    "    demo_path = OUTPUT_DIR / \"demo_result.json\"\n",
    "    demo_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(demo_path, \"w\") as f:\n",
    "        json.dump(result, f, indent=2, default=str)\n",
    "    print(f\"\\nğŸ“ Full result saved to {demo_path}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No validation images found for demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6062a7",
   "metadata": {},
   "source": [
    "## Step 6 Â· Full Evaluation (Defense Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3227e9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Step 6: Evaluation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "evaluator = GeoExtractEvaluator(CFG)\n",
    "yolo_weights = YOLO_CKPT_DIR / \"building_detector\" / \"weights\" / \"best.pt\"\n",
    "\n",
    "all_metrics = evaluator.run_full_evaluation(\n",
    "    dataset_yaml=dataset_yaml,\n",
    "    yolo_weights=yolo_weights,\n",
    "    pipeline=pipeline,\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ“ Full report: {EVAL_DIR / 'full_evaluation_report.json'}\")\n",
    "log_disk(\"After evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc4ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Final cleanup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "pipeline.cleanup()\n",
    "del pipeline\n",
    "nuke_vram()\n",
    "log_disk(\"Final state\")\n",
    "print(\"\\nğŸ‰ GeoExtract v2 pipeline complete!\")\n",
    "print(f\"ğŸ“‚ All outputs saved to: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb05a9b",
   "metadata": {},
   "source": [
    "## ğŸ’¾ Step 7 Â· Export Weights & Persist Outputs\n",
    "\n",
    "Kaggle deletes `/kaggle/working` when a session ends.  \n",
    "**Run this cell to zip all trained weights into a Kaggle output artifact.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5b8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile, shutil\n",
    "\n",
    "export_dir = OUTPUT_DIR / \"kaggle_export\"\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€ 1. Zip YOLO best weights â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "yolo_best = YOLO_CKPT_DIR / \"building_detector\" / \"weights\" / \"best.pt\"\n",
    "if yolo_best.exists():\n",
    "    shutil.copy2(yolo_best, export_dir / \"yolo_best.pt\")\n",
    "    print(f\"âœ… YOLO weights copied ({yolo_best.stat().st_size / 1e6:.1f} MB)\")\n",
    "else:\n",
    "    print(\"âš ï¸  YOLO best.pt not found â€” skipping.\")\n",
    "\n",
    "# â”€â”€ 2. Zip VLM LoRA adapter â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vlm_adapter = VLM_CKPT_DIR / \"geoextract-vlm-lora\"\n",
    "if vlm_adapter.exists():\n",
    "    zip_path = export_dir / \"vlm_lora_adapter.zip\"\n",
    "    with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as zf:\n",
    "        for fp in vlm_adapter.rglob(\"*\"):\n",
    "            if fp.is_file():\n",
    "                zf.write(fp, fp.relative_to(vlm_adapter.parent))\n",
    "    print(f\"âœ… VLM LoRA adapter zipped ({zip_path.stat().st_size / 1e6:.1f} MB)\")\n",
    "else:\n",
    "    print(\"âš ï¸  VLM adapter not found â€” skipping.\")\n",
    "\n",
    "# â”€â”€ 3. Copy evaluation reports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for report in EVAL_DIR.glob(\"*.json\"):\n",
    "    shutil.copy2(report, export_dir / report.name)\n",
    "for img in EVAL_DIR.glob(\"*.png\"):\n",
    "    shutil.copy2(img, export_dir / img.name)\n",
    "print(f\"âœ… Evaluation artifacts copied to {export_dir}\")\n",
    "\n",
    "# â”€â”€ 4. Final manifest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "manifest = {\n",
    "    \"project\": \"GeoExtract v2\",\n",
    "    \"yolo_model\": \"YOLOv11-nano\",\n",
    "    \"vlm_model\": \"Qwen2-VL-2B-Instruct + LoRA\",\n",
    "    \"yolo_epochs\": CFG.yolo.epochs,\n",
    "    \"vlm_epochs\": CFG.vlm.epochs,\n",
    "    \"max_yolo_samples\": CFG.data.max_yolo_samples,\n",
    "    \"max_vlm_samples\": CFG.data.max_vlm_samples,\n",
    "    \"files\": [str(p.relative_to(export_dir)) for p in export_dir.rglob(\"*\") if p.is_file()],\n",
    "}\n",
    "with open(export_dir / \"manifest.json\", \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "log_disk(\"After export\")\n",
    "print(f\"\\nğŸ“¦ Export complete â†’ {export_dir}\")\n",
    "print(\"   Kaggle will persist everything under /kaggle/working as 'Output'.\")\n",
    "print(\"   Download the 'kaggle_export' folder from the Output tab.\")\n",
    "print(\"\\nâ”€â”€ To push VLM adapter to ğŸ¤— HuggingFace Hub â”€â”€\")\n",
    "print(\"   from huggingface_hub import HfApi\")\n",
    "print(\"   api = HfApi()\")\n",
    "print(\"   api.upload_folder(folder_path='vlm_lora_adapter', repo_id='YOUR_USER/geoextract-vlm-lora')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e8cc1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## â±ï¸ Estimated Training Times (NVIDIA T4 16 GB â€” Kaggle)\n",
    "\n",
    "| Step | Estimated Time | Key Settings |\n",
    "|------|---------------|-------------|\n",
    "| **Data Processing** | 5â€“15 min | max 300 YOLO samples, JPEG output |\n",
    "| **QA Generation** | 5â€“10 min | max 300 VLM samples |\n",
    "| **YOLO Training** (30 epochs) | 45 min â€“ 1.5 h | batch=8, img=640, YOLOv11-nano |\n",
    "| **VLM Training** (3 epochs) | 2â€“3 hours | batch=1, grad_accum=16, LoRA 4-bit, eager attn |\n",
    "| **Inference Demo** | 1â€“2 min | Single image end-to-end |\n",
    "| **Evaluation** | 10â€“20 min | max 50 eval samples |\n",
    "| **Export** | < 1 min | Zip weights + reports |\n",
    "| **Total** | **~4â€“6 hours** | âœ… Within Kaggle 12-hour limit |\n",
    "\n",
    "### ğŸ’¡ Tips\n",
    "- Set `CFG.data.max_yolo_samples = 100` for a fast debug run\n",
    "- Reduce YOLO epochs: `CFG.yolo.epochs = 10`\n",
    "- Reduce VLM epochs: `CFG.vlm.epochs = 1`\n",
    "- Pipeline auto-resumes from checkpoints if Kaggle session restarts\n",
    "- `log_disk()` / `log_vram()` â€” call anytime to check resource usage\n",
    "- All outputs are under `/kaggle/working/geoextract_output/kaggle_export/`"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
